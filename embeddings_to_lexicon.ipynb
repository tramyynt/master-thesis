{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import glob\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import zipfile\n",
    "import ast\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc2 = pd.read_excel('/home_remote/dic_avg100_annotated_official.xlsx')\n",
    "liwc2['Terms'] = liwc2['Term'].apply(lambda x: ast.literal_eval(x))\n",
    "result = dict(zip(liwc2['Category'], liwc2['Terms']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dict(_dict):\n",
    "    '''\n",
    "    param: dictionary with key as categorie and value as the list of words\n",
    "    return: a lexicon with key as word and values as the list of categories the word belongs to.\n",
    "    '''\n",
    "    lt =[]\n",
    "    for x in _dict.values():\n",
    "        lt = lt+x\n",
    "    mylist = [*set(lt)]\n",
    "    lexicon = {}\n",
    "    for i in mylist:\n",
    "        ls = []\n",
    "        for j in _dict.keys():\n",
    "            #print(j)\n",
    "            if i in _dict[j]:\n",
    "                ls.append(j)\n",
    "        lexicon[i] = ls\n",
    "    return lexicon, list(_dict.keys())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://github.com/chbrown/liwc-python\n",
    "def build_trie(lexicon):\n",
    "    \"\"\"\n",
    "    A `*` indicates a wildcard match.\n",
    "    \"\"\"\n",
    "    trie = {}\n",
    "    for pattern, category_names in lexicon.items():\n",
    "        cursor = trie\n",
    "        for char in pattern:\n",
    "            if char == \"*\":\n",
    "                cursor[\"*\"] = category_names\n",
    "                break\n",
    "            if char not in cursor:\n",
    "                cursor[char] = {}\n",
    "            cursor = cursor[char]\n",
    "        cursor[\"$\"] = category_names\n",
    "    return trie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ref: https://github.com/chbrown/liwc-python\n",
    "def search_trie(trie, token, token_i=0):\n",
    "    if \"*\" in trie:\n",
    "        return trie[\"*\"]\n",
    "    if \"$\" in trie and token_i == len(token):\n",
    "        return trie[\"$\"]\n",
    "    if token_i < len(token):\n",
    "        char = token[token_i]\n",
    "        if char in trie:\n",
    "            return search_trie(trie[char], token, token_i + 1)\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://github.com/chbrown/liwc-python\n",
    "def load_token_parser(dic):\n",
    "    lexicon, category_names = parse_dict(dic)\n",
    "    trie = build_trie(lexicon)\n",
    "\n",
    "    def parse_token(token):\n",
    "        for category_name in search_trie(trie, token):\n",
    "            yield category_name\n",
    "\n",
    "    return parse_token, category_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S+')\n",
    "    for match in tokenizer.tokenize(text) :\n",
    "        yield match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(input_, result):\n",
    "   # dic['Terms'] = dic['Term'].apply(lambda x: ast.literal_eval(x))\n",
    "   # result = dict(zip(dic['Category'], dic['Terms']))\n",
    "    #lexicon = parse_dict(result)\n",
    "    parse, category_names = load_token_parser(result)\n",
    "    input_tokens = tokenize(input_.lower())\n",
    "    counts = Counter(category for token in input_tokens for category in parse(token))\n",
    "    \n",
    "#   a = pd.DataFrame.from_dict(dict(counts),orient = 'index').reset_index()\n",
    "#   a= a.rename(columns={\"index\": \"Category\", 0: \"Count\"})\n",
    "#   a['Percentage(%)']= round(a['Count']/len(input_.split(' '))*100,2)\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIWC alike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "%store -r df\n",
    "%store -r df_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine Text and Title\n",
    "df['Text_Title'] = df['Title'] + '.' + df['Text']\n",
    "df_neg['Text_Title'] = df_neg['Title'] + '.' + df_neg['Text']\n",
    "#combine all text of same user into one string\n",
    "text_per_user_pos = df.groupby('User')['Text_Title'].apply(' '.join).reset_index()\n",
    "text_per_user_neg = df_neg.groupby('User')['Text_Title'].apply(' '.join).reset_index()\n",
    "#add label\n",
    "text_per_user_pos['label'] = 1\n",
    "text_per_user_neg['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an empty dict\n",
    "avg_pos = {}\n",
    "for i in df['User'].unique():\n",
    "    temp = df[df['User']==i]\n",
    "    #create an empty vector with length of number of post of the user\n",
    "    vec = []\n",
    "    for j in temp['Text_Title']:\n",
    "        vc = np.zeros(len(liwc2['Category']))\n",
    "        tp = main(j, liwc2)\n",
    "        tp = np.array(list(tp.values()))/len(j.split(' '))\n",
    "        for k in range(len(tp)):\n",
    "            vc[k] = tp[k]\n",
    "        # insert vc to vec\n",
    "        vec.append(vc)\n",
    "    vec_avg = np.mean(vec, axis=0)\n",
    "    #append the vector to the dictionary with key as user id and value as vector\n",
    "    avg_pos[i] = vec_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input text to LIWC-alike\n",
    "liwc_pos = [main(text, result) for text in text_per_user_pos['Text_Title']]\n",
    "liwc_neg = [main(text, result) for text in text_per_user_neg['Text_Title']]\n",
    "#then buid a dictionary with key as user, value as a list of LIWC features\n",
    "liwc_pos_dict = {}\n",
    "liwc_neg_dict = {}\n",
    "for i in range(len(liwc_pos)):\n",
    "    liwc_pos_dict[text_per_user_pos['User'][i]] = liwc_pos[i]\n",
    "for i in range(len(liwc_neg)):\n",
    "    liwc_neg_dict[text_per_user_neg['User'][i]] = liwc_neg[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate percentage of each LIWC feature per user\n",
    "liwc_pos_percent = {}\n",
    "liwc_neg_percent = {}\n",
    "for key, value in liwc_pos_dict.items():\n",
    "    liwc_pos_percent[key] = {k: v / sum(value.values()) for k, v in value.items()}\n",
    "for key, value in liwc_neg_dict.items():\n",
    "    liwc_neg_percent[key] = {k: v / sum(value.values()) for k, v in value.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation of LIWC features with label\n",
    "liwc_pos_percent_df = pd.DataFrame(liwc_pos_percent).T\n",
    "liwc_neg_percent_df = pd.DataFrame(liwc_neg_percent).T\n",
    "liwc_pos_percent_df['label'] = 1\n",
    "liwc_neg_percent_df['label'] = 0\n",
    "liwc_percent_df = pd.concat([liwc_pos_percent_df, liwc_neg_percent_df])\n",
    "#liwc_percent_df = liwc_percent_df.fillna(0)\n",
    "corr = liwc_percent_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose the top 15 most correlated features to label\n",
    "corr_label = corr['label'].sort_values(ascending=False)\n",
    "corr_label = corr_label[1:16]\n",
    "corr_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the name of top 15 features\n",
    "liwc_features = list(corr_label.index)\n",
    "#concat with label\n",
    "liwc_features.append('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "corr_liwc = liwc_percent_df[liwc_features].corr()\n",
    "sns.heatmap(corr_liwc, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
