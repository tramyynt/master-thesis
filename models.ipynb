{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk import word_tokenize, ngrams\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import string\n",
    "import gensim\n",
    "import joblib\n",
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "HOME_DIR = \"/home_remote\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives_df_path = os.path.join(HOME_DIR, \"positive_df.pkl\")\n",
    "negatives_df_path = os.path.join(HOME_DIR, \"negative_df.pkl\")\n",
    "\n",
    "positives = pd.read_pickle(positives_df_path)\n",
    "negatives = pd.read_pickle(negatives_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join tiltle and text\n",
    "positives['text'] = positives['Title'] + positives['Text']\n",
    "negatives['text'] = negatives['Title'] + negatives['Text']\n",
    "#join all text of the same user\n",
    "pos = positives.groupby('TrainSubjectId')['text'].apply(' '.join).reset_index()\n",
    "neg = negatives.groupby('TrainSubjectId')['text'].apply(' '.join).reset_index()\n",
    "#Labelling the data\n",
    "pos['Label'] = 1\n",
    "neg['Label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TrainSubjectId</th>\n",
       "      <th>text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_subject9956</td>\n",
       "      <td>But killing him themselves will give them ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_subject8693</td>\n",
       "      <td>Can...can i put my dick in it?      You're...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_subject7819</td>\n",
       "      <td>Psychologists Shielded U.S. Torture Program, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_subject5510</td>\n",
       "      <td>wtf Hull are in the premiership      I sno...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_subject489</td>\n",
       "      <td>Is it you, keyboard cat?      What a time ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>train_subject7049</td>\n",
       "      <td>That line cracked me the fuck up.  I just ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>train_subject6802</td>\n",
       "      <td>Yeah i use this: http://imgur.com/TOvn6ij ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>train_subject3902</td>\n",
       "      <td>My first ever needle felting creation! :D (Bu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>train_subject5807</td>\n",
       "      <td>Scientists make breakthrough in understanding...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>train_subject5671</td>\n",
       "      <td>Pretty sure it's going to happen to me whe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>486 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        TrainSubjectId                                               text  \\\n",
       "0    train_subject9956      But killing him themselves will give them ...   \n",
       "1    train_subject8693      Can...can i put my dick in it?      You're...   \n",
       "2    train_subject7819   Psychologists Shielded U.S. Torture Program, ...   \n",
       "3    train_subject5510      wtf Hull are in the premiership      I sno...   \n",
       "4     train_subject489      Is it you, keyboard cat?      What a time ...   \n",
       "..                 ...                                                ...   \n",
       "481  train_subject7049      That line cracked me the fuck up.  I just ...   \n",
       "482  train_subject6802      Yeah i use this: http://imgur.com/TOvn6ij ...   \n",
       "483  train_subject3902   My first ever needle felting creation! :D (Bu...   \n",
       "484  train_subject5807   Scientists make breakthrough in understanding...   \n",
       "485  train_subject5671      Pretty sure it's going to happen to me whe...   \n",
       "\n",
       "     Label  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  \n",
       "..     ...  \n",
       "481      0  \n",
       "482      0  \n",
       "483      1  \n",
       "484      0  \n",
       "485      0  \n",
       "\n",
       "[486 rows x 3 columns]"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#concatenate the data\n",
    "data = pd.concat([pos, neg], ignore_index=True)\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(document):\n",
    "\n",
    "        # Remove extra white space from text\n",
    "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "         \n",
    "        # Remove all the special characters from text\n",
    "        document = re.sub(r'\\W', ' ', str(document))\n",
    " \n",
    "        # Remove all single characters from text\n",
    "        #document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    " \n",
    "        # Converting to lowercase\n",
    "        document = document.lower()\n",
    "\n",
    "        return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-processing for tfidf\n",
    "def clean_text(text):\n",
    "    # lower text\n",
    "    text = text.lower()\n",
    "     #text = nltk.word_tokenize(text)\n",
    "    text = [word.strip(string.punctuation) for word in text.split(\" \")]\n",
    "    # remove stop words\n",
    "    stop = stopwords.words('english')\n",
    "    text = [x for x in text if x not in stop]\n",
    "    # remove empty tokens\n",
    "    text = [t for t in text if len(t) > 0]\n",
    "    # remove words with only one letter\n",
    "    text = [t for t in text if len(t) > 1]\n",
    "    # join all text\n",
    "    text = ' '.join(text)\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf-idf\n",
    "def feature_extract(text, type):\n",
    "    if type == 'tfidf':\n",
    "        tfidfconverter = TfidfVectorizer(max_features=1000, min_df=5, max_df=0.7)\n",
    "        X = tfidfconverter.fit_transform(text).toarray()\n",
    "    elif type == 'doc2vec':\n",
    "        pass\n",
    "    return X "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data\n",
    "#label data for positive and negative\n",
    "positives['Label'] = 1\n",
    "negatives['Label'] = 0\n",
    "#concatenate the data positive and negatives\n",
    "train = pd.concat([positives, negatives], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(df, tokens_only=False):\n",
    "    for i, line in enumerate(df['text']):\n",
    "        tokens = gensim.utils.simple_preprocess(line)\n",
    "        if tokens_only:\n",
    "            yield tokens\n",
    "        else:\n",
    "            # For training data, add tags\n",
    "            yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
    "\n",
    "train_corpus = list(read_corpus(train))\n",
    "#test_corpus = list(read_corpus(lee_test_file, tokens_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(documents= train_corpus, dm = 1, vector_size=100, min_count=1, epochs=10, window=10, negative= 20,  alpha=0.025,min_alpha=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2= gensim.models.doc2vec.Doc2Vec(documents= train_corpus, dm = 0, vector_size=100, min_count=1, epochs=10, window=10, sample=1e-4,hs =1,  alpha=0.025,min_alpha=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map train['Vector'] to train_corpus\n",
    "train['Tag'] = train_corpus\n",
    "#get tags of train_corpus\n",
    "tags = [x.tags[0] for x in train_corpus]\n",
    "train['Vector']= [np.concatenate((model.dv[x], model2.dv[x]), axis=None) for x in tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average of vectors for each user, including the label of user\n",
    "a = train.groupby('TrainSubjectId').agg({'Vector': 'mean', 'Label': 'first'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home_remote/lg2.pkl']"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#joblib a model\n",
    "joblib.dump(lg2, os.path.join(HOME_DIR,'lg2.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, y):\n",
    "\n",
    "    w = [1, 2**1, 2**2, 2**3, 2**4, 2**5, 2**6,2**7, 2**8]\n",
    "    weight = [{0: 1/(1+x),  1: x/(1+x)} for x in w]\n",
    "    C = [2**-6, 2**-5, 2**-4, 2**-3, 2**-2, 2**-1, 1, 2**1, 2**2, 2**3, 2**4, 2**5, 2**6]\n",
    "    # define grid search\n",
    "    hyperparam_grid = {\"class_weight\": weight\n",
    "                    ,\"penalty\": [\"l1\", \"l2\"]\n",
    "                    ,\"C\": C\n",
    "                    ,\"fit_intercept\": [True, False]  }\n",
    "    # define evaluation procedure\n",
    "    cv = KFold(n_splits=10, shuffle=True, random_state=13)\n",
    "    # define grid search\n",
    "    model_test = LogisticRegression(solver='liblinear')\n",
    "    grid = GridSearchCV(estimator=model_test, param_grid=hyperparam_grid, cv=cv, scoring='roc_auc')\n",
    "    grid_result = grid.fit(X, y)\n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "    #build a model with the best parameters, fix class_weight = (0.2, 0.8)\n",
    "\n",
    "    model = LogisticRegression(**grid_result.best_params_)\n",
    "    model.fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.862 (0.069)\n",
      "Accuracy: 0.8868312757201646\n",
      "Precision: 0.6944444444444444\n",
      "Recall: 0.6024096385542169\n",
      "F1: 0.6451612903225806\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF\n",
    "text = data['text'].apply(clean_text)\n",
    "X_tfidf = feature_extract(text, 'tfidf')\n",
    "y_tfidf = data['Label']\n",
    "\n",
    "lg1 = LogisticRegression(C=4, class_weight={0: 0.2, 1: 0.8}, fit_intercept=True, penalty='l1', solver='liblinear')\n",
    "y_pred = cross_val_predict(lg1, X_tfidf, y_tfidf, cv=cv)\n",
    "#dataframe of y_pred and y\n",
    "lg1_train = pd.DataFrame({'Actual': y_tfidf, 'Predicted': y_pred})\n",
    "\n",
    "result = cross_val_score(lg1, X_tfidf, y_tfidf, cv=cv, scoring='roc_auc')\n",
    "print(\"AUC: %.3f (%.3f)\" % (result.mean(), result.std()))\n",
    "print(\"Accuracy:\",accuracy_score(y_tfidf, y_pred))\n",
    "print(\"Precision:\",precision_score(y_tfidf, y_pred))\n",
    "print(\"Recall:\",recall_score(y_tfidf, y_pred))\n",
    "print(\"F1:\",f1_score(y_tfidf, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.996927 using {'C': 32, 'class_weight': {0: 0.3333333333333333, 1: 0.6666666666666666}, 'fit_intercept': True, 'penalty': 'l2'}\n",
      "AUC: 0.997 (0.004)\n",
      "Accuracy: 0.9732510288065843\n",
      "Precision: 0.9605263157894737\n",
      "Recall: 0.8795180722891566\n",
      "F1: 0.9182389937106917\n"
     ]
    }
   ],
   "source": [
    "#Doc2Vec\n",
    "X_doc2vec = a['Vector'].tolist()\n",
    "y_doc2vec = a['Label'].tolist()\n",
    "\n",
    "lg2 = logistic_regression(X_doc2vec, y_doc2vec)\n",
    "y_pred_doc2vc = cross_val_predict(lg2, X_doc2vec, y_doc2vec, cv=cv)\n",
    "#dataframe of y_pred and y\n",
    "lg2_train = pd.DataFrame({'Actual': y_doc2vec, 'Predicted': y_pred_doc2vc})\n",
    "\n",
    "result_doc2vec = cross_val_score(lg2, X_doc2vec, y_doc2vec, cv=cv, scoring='roc_auc')\n",
    "print(\"AUC: %.3f (%.3f)\" % (result_doc2vec.mean(), result_doc2vec.std()))\n",
    "print(\"Accuracy:\",accuracy_score(y_doc2vec, y_pred_doc2vc))\n",
    "print(\"Precision:\",precision_score(y_doc2vec, y_pred_doc2vc))\n",
    "print(\"Recall:\",recall_score(y_doc2vec, y_pred_doc2vc))\n",
    "print(\"F1:\",f1_score(y_doc2vec, y_pred_doc2vc))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg1 = LogisticRegression(C=4, class_weight={0: 0.2, 1: 0.8}, fit_intercept=True, penalty='l1', solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=4, class_weight={0: 0.2, 1: 0.8}, penalty='l1',\n",
       "                   solver='liblinear')"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg1.fit(X_tfidf, y_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home_remote/lg1.pkl']"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(lg1, os.path.join(HOME_DIR,'lg1.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.996927 using {'C': 32, 'class_weight': {0: 0.3333333333333333, 1: 0.6666666666666666}, 'fit_intercept': True, 'penalty': 'l2'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=32,\n",
       "                   class_weight={0: 0.3333333333333333, 1: 0.6666666666666666})"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg2= logistic_regression(X_doc2vec, y_doc2vec)\n",
    "lg2.fit(X_doc2vec, y_doc2vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home_remote/lg2.pkl']"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(lg2, os.path.join(HOME_DIR,'lg2.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home_remote/model2_doc2vec.pkl']"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#joblib a model\n",
    "joblib.dump(model, os.path.join(HOME_DIR,'model_doc2vec.pkl'))\n",
    "joblib.dump(model2, os.path.join(HOME_DIR,'model2_doc2vec.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "fname = get_tmpfile(os.path.join(HOME_DIR,\"master_thesis/model_evaluation/my_doc2vec_model\"))\n",
    "model.save(fname)\n",
    "fname2 = get_tmpfile(os.path.join(HOME_DIR,\"master_thesis/model_evaluation/my_doc2vec_model2\"))\n",
    "model2.save(fname2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = train['text'][0:3].str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.4728922e-03,  1.1294025e-03, -2.4310261e-04, -9.6121192e-04,\n",
       "        4.7670985e-03,  1.4386200e-03, -3.0160856e-03, -4.8014042e-03,\n",
       "        1.2395275e-03,  2.8639727e-03,  4.8470020e-04,  1.5936660e-03,\n",
       "       -3.5161019e-04, -4.6375393e-05,  3.9771260e-03,  4.8645083e-03,\n",
       "       -6.5763446e-04,  8.5542200e-04,  3.2963913e-03, -1.0223204e-03,\n",
       "        1.3308584e-03, -3.0275257e-04,  3.9787532e-04, -3.4252019e-03,\n",
       "        4.0295329e-03,  4.9934722e-03, -2.5388356e-03,  3.1203639e-03,\n",
       "        3.1022662e-03,  2.1049499e-03,  2.4390297e-03, -4.6553402e-03,\n",
       "       -1.4479510e-03,  5.9771957e-04, -1.0302877e-03, -4.6756309e-03,\n",
       "       -3.4628245e-03,  4.5805462e-03,  1.5165460e-03, -1.1353013e-03,\n",
       "       -9.7143231e-04,  3.7325716e-03,  1.4615315e-03, -4.1971123e-03,\n",
       "       -9.2977466e-04, -2.8820890e-03, -2.9301124e-03, -1.8280179e-03,\n",
       "       -3.2186517e-03, -7.8540383e-04, -6.8161875e-04,  8.2067726e-04,\n",
       "        3.9903154e-03,  1.3948452e-03,  4.7700340e-03,  1.1494773e-03,\n",
       "        4.2567230e-03, -2.2238069e-03,  3.5481434e-03, -2.8138160e-04,\n",
       "       -1.4928820e-03,  4.6760468e-03, -3.0061046e-03,  3.1790298e-03,\n",
       "       -1.5946251e-03, -4.6265083e-03, -5.3862273e-04,  3.1345307e-03,\n",
       "        1.4858037e-03, -8.2761348e-05,  2.0986318e-03,  4.9346092e-04,\n",
       "       -4.2524375e-03,  2.4418521e-03, -3.6324388e-03,  5.8381556e-05,\n",
       "        4.1345907e-03,  2.7707266e-03, -3.4056532e-03,  1.8296666e-03,\n",
       "       -4.1672522e-03, -2.8554338e-03,  1.2312937e-03,  1.4280170e-03,\n",
       "       -1.1389068e-03,  2.2863590e-03,  3.6783509e-03, -4.6907896e-03,\n",
       "       -1.4834574e-03, -4.5069489e-03,  1.2839037e-03, -3.3452129e-03,\n",
       "        4.8444998e-03, -2.2095719e-03,  3.8788808e-03,  2.0167148e-03,\n",
       "        3.4807974e-03, -3.6942705e-03, -8.3193067e-04,  4.1481853e-05],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.infer_vector(read_corpus(tt, tokens_only=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec = joblib.load(os.path.join(HOME_DIR, \"lg2.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00160664])"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec.predict_proba(X_doc2vec[0].reshape(1, -1))[:,1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
