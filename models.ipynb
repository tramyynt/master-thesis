{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk import word_tokenize, ngrams\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import string\n",
    "import gensim\n",
    "import joblib\n",
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "HOME_DIR = \"/home_remote\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives_df_path = os.path.join(HOME_DIR, \"positive_df.pkl\")\n",
    "negatives_df_path = os.path.join(HOME_DIR, \"negative_df.pkl\")\n",
    "\n",
    "positives = pd.read_pickle(positives_df_path)\n",
    "negatives = pd.read_pickle(negatives_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join tiltle and text\n",
    "positives['text'] = positives['Title'] + positives['Text']\n",
    "negatives['text'] = negatives['Title'] + negatives['Text']\n",
    "#join all text of the same user\n",
    "pos = positives.groupby('TrainSubjectId')['text'].apply(' '.join).reset_index()\n",
    "neg = negatives.groupby('TrainSubjectId')['text'].apply(' '.join).reset_index()\n",
    "#Labelling the data\n",
    "pos['Label'] = 1\n",
    "neg['Label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TrainSubjectId</th>\n",
       "      <th>text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_subject4454</td>\n",
       "      <td>Illegal aliens suspected in murder of nurse s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_subject8344</td>\n",
       "      <td>Well it's not as if they really need to tr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_subject9884</td>\n",
       "      <td>I think everybody does. Personally I can't...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_subject9201</td>\n",
       "      <td>Or a golden tuffalo.      I think the real...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_subject8559</td>\n",
       "      <td>Little 7 year old Daisy having a nap     For ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>train_subject2418</td>\n",
       "      <td>TIFU by making a 'shitty' joke at work  I jus...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>train_subject1879</td>\n",
       "      <td>How did you get out?\\nalso, congratulation...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>train_subject6188</td>\n",
       "      <td>A U.S.-led coalition dropped new leaflets ove...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>train_subject1839</td>\n",
       "      <td>I did. I was always an incredibly morose, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>train_subject541</td>\n",
       "      <td>Voltaire      Atheist here. \\n\\nNo. No I d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>486 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        TrainSubjectId                                               text  \\\n",
       "0    train_subject4454   Illegal aliens suspected in murder of nurse s...   \n",
       "1    train_subject8344      Well it's not as if they really need to tr...   \n",
       "2    train_subject9884      I think everybody does. Personally I can't...   \n",
       "3    train_subject9201      Or a golden tuffalo.      I think the real...   \n",
       "4    train_subject8559   Little 7 year old Daisy having a nap     For ...   \n",
       "..                 ...                                                ...   \n",
       "481  train_subject2418   TIFU by making a 'shitty' joke at work  I jus...   \n",
       "482  train_subject1879      How did you get out?\\nalso, congratulation...   \n",
       "483  train_subject6188   A U.S.-led coalition dropped new leaflets ove...   \n",
       "484  train_subject1839      I did. I was always an incredibly morose, ...   \n",
       "485   train_subject541      Voltaire      Atheist here. \\n\\nNo. No I d...   \n",
       "\n",
       "     Label  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  \n",
       "..     ...  \n",
       "481      0  \n",
       "482      1  \n",
       "483      0  \n",
       "484      1  \n",
       "485      0  \n",
       "\n",
       "[486 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#concatenate the data\n",
    "data = pd.concat([pos, neg], ignore_index=True)\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(document):\n",
    "\n",
    "        # Remove extra white space from text\n",
    "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "         \n",
    "        # Remove all the special characters from text\n",
    "        document = re.sub(r'\\W', ' ', str(document))\n",
    " \n",
    "        # Remove all single characters from text\n",
    "        #document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    " \n",
    "        # Converting to lowercase\n",
    "        document = document.lower()\n",
    "\n",
    "        return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-processing for tfidf\n",
    "def clean_text(text):\n",
    "    # lower text\n",
    "    text = text.lower()\n",
    "     #text = nltk.word_tokenize(text)\n",
    "    text = [word.strip(string.punctuation) for word in text.split(\" \")]\n",
    "    # remove stop words\n",
    "    stop = stopwords.words('english')\n",
    "    text = [x for x in text if x not in stop]\n",
    "    # remove empty tokens\n",
    "    text = [t for t in text if len(t) > 0]\n",
    "    # remove words with only one letter\n",
    "    text = [t for t in text if len(t) > 1]\n",
    "    # join all text\n",
    "    text = ' '.join(text)\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf-idf\n",
    "def feature_extract(text, type):\n",
    "    if type == 'tfidf':\n",
    "        tfidfconverter = TfidfVectorizer(max_features=1000, max_df=0.7, min_df=0.1)\n",
    "        X = tfidfconverter.fit_transform(text).toarray()\n",
    "    elif type == 'doc2vec':\n",
    "        pass\n",
    "    return X, tfidfconverter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data\n",
    "#label data for positive and negative\n",
    "positives['Label'] = 1\n",
    "negatives['Label'] = 0\n",
    "#concatenate the data positive and negatives\n",
    "train = pd.concat([positives, negatives], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(df, tokens_only=False):\n",
    "    for i, line in enumerate(df['text']):\n",
    "        tokens = gensim.utils.simple_preprocess(line)\n",
    "        if tokens_only:\n",
    "            yield tokens\n",
    "        else:\n",
    "            # For training data, add tags\n",
    "            yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
    "\n",
    "train_corpus = list(read_corpus(train))\n",
    "#test_corpus = list(read_corpus(lee_test_file, tokens_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(documents= train_corpus, dm = 1, vector_size=100, min_count=1, epochs=10, window=10, negative= 20,  alpha=0.025,min_alpha=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2= gensim.models.doc2vec.Doc2Vec(documents= train_corpus, dm = 0, vector_size=100, min_count=1, epochs=10, window=10, sample=1e-4,hs =1,  alpha=0.025,min_alpha=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map train['Vector'] to train_corpus\n",
    "train['Tag'] = train_corpus\n",
    "#get tags of train_corpus\n",
    "tags = [x.tags[0] for x in train_corpus]\n",
    "train['Vector']= [np.concatenate((model.dv[x], model2.dv[x]), axis=None) for x in tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average of vectors for each user, including the label of user\n",
    "a = train.groupby('TrainSubjectId').agg({'Vector': 'mean', 'Label': 'first'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_doc2vec = np.array(a['Vector'].tolist())\n",
    "y_doc2vec = np.array(a['Label'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home_remote/lg2.pkl']"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#joblib a model\n",
    "joblib.dump(lg2, os.path.join(HOME_DIR,'lg2.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, y):\n",
    "\n",
    "    w = [1, 2**1, 2**2, 2**3, 2**4, 2**5, 2**6,2**7, 2**8]\n",
    "    weight = [{0: 1/(1+x),  1: x/(1+x)} for x in w]\n",
    "    C = [2**-6, 2**-5, 2**-4, 2**-3, 2**-2, 2**-1, 1, 2**1, 2**2, 2**3, 2**4, 2**5, 2**6]\n",
    "    # define grid search\n",
    "    hyperparam_grid = {\"class_weight\": weight\n",
    "                    ,\"penalty\": [\"l1\", \"l2\"]\n",
    "                    ,\"C\": C\n",
    "                    ,\"fit_intercept\": [True, False]  }\n",
    "    # define evaluation procedure\n",
    "    cv = KFold(n_splits=10, shuffle=True, random_state=13)\n",
    "    # define grid search\n",
    "    model_test = LogisticRegression(solver='liblinear')\n",
    "    grid = GridSearchCV(estimator=model_test, param_grid=hyperparam_grid, cv=cv, scoring='roc_auc')\n",
    "    grid_result = grid.fit(X, y)\n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "    #build a model with the best parameters, fix class_weight = (0.2, 0.8)\n",
    "\n",
    "    model = LogisticRegression(**grid_result.best_params_)\n",
    "    model.fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.841 (0.087)\n",
      "Accuracy: 0.8868312757201646\n",
      "Precision: 0.6891891891891891\n",
      "Recall: 0.6144578313253012\n",
      "F1: 0.6496815286624203\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF\n",
    "text = data['text'].apply(clean_text)\n",
    "X_tfidf, tfidf_model = feature_extract(text, 'tfidf')\n",
    "y_tfidf = data['Label']\n",
    "\n",
    "lg1 = LogisticRegression(C=4, class_weight={0: 0.2, 1: 0.8}, fit_intercept=True, penalty='l1', solver='liblinear')\n",
    "y_pred = cross_val_predict(lg1, X_tfidf, y_tfidf, cv=cv)\n",
    "#dataframe of y_pred and y\n",
    "lg1_train = pd.DataFrame({'Actual': y_tfidf, 'Predicted': y_pred})\n",
    "\n",
    "result = cross_val_score(lg1, X_tfidf, y_tfidf, cv=cv, scoring='roc_auc')\n",
    "print(\"AUC: %.3f (%.3f)\" % (result.mean(), result.std()))\n",
    "print(\"Accuracy:\",accuracy_score(y_tfidf, y_pred))\n",
    "print(\"Precision:\",precision_score(y_tfidf, y_pred))\n",
    "print(\"Recall:\",recall_score(y_tfidf, y_pred))\n",
    "print(\"F1:\",f1_score(y_tfidf, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.996927 using {'C': 32, 'class_weight': {0: 0.3333333333333333, 1: 0.6666666666666666}, 'fit_intercept': True, 'penalty': 'l2'}\n",
      "AUC: 0.997 (0.004)\n",
      "Accuracy: 0.9732510288065843\n",
      "Precision: 0.9605263157894737\n",
      "Recall: 0.8795180722891566\n",
      "F1: 0.9182389937106917\n"
     ]
    }
   ],
   "source": [
    "#Doc2Vec\n",
    "X_doc2vec = a['Vector'].tolist()\n",
    "y_doc2vec = a['Label'].tolist()\n",
    "\n",
    "lg2 = logistic_regression(X_doc2vec, y_doc2vec)\n",
    "y_pred_doc2vc = cross_val_predict(lg2, X_doc2vec, y_doc2vec, cv=cv)\n",
    "#dataframe of y_pred and y\n",
    "lg2_train = pd.DataFrame({'Actual': y_doc2vec, 'Predicted': y_pred_doc2vc})\n",
    "\n",
    "result_doc2vec = cross_val_score(lg2, X_doc2vec, y_doc2vec, cv=cv, scoring='roc_auc')\n",
    "print(\"AUC: %.3f (%.3f)\" % (result_doc2vec.mean(), result_doc2vec.std()))\n",
    "print(\"Accuracy:\",accuracy_score(y_doc2vec, y_pred_doc2vc))\n",
    "print(\"Precision:\",precision_score(y_doc2vec, y_pred_doc2vc))\n",
    "print(\"Recall:\",recall_score(y_doc2vec, y_pred_doc2vc))\n",
    "print(\"F1:\",f1_score(y_doc2vec, y_pred_doc2vc))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg1 = LogisticRegression(C=4, class_weight={0: 0.2, 1: 0.8}, fit_intercept=True, penalty='l1', solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=4, class_weight={0: 0.2, 1: 0.8}, penalty='l1',\n",
       "                   solver='liblinear')"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg1.fit(X_tfidf, y_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.996927 using {'C': 32, 'class_weight': {0: 0.3333333333333333, 1: 0.6666666666666666}, 'fit_intercept': True, 'penalty': 'l2'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=32,\n",
       "                   class_weight={0: 0.3333333333333333, 1: 0.6666666666666666})"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg2= logistic_regression(X_doc2vec, y_doc2vec)\n",
    "lg2.fit(X_doc2vec, y_doc2vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home_remote/lg2.pkl']"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(lg2, os.path.join(HOME_DIR,'lg2.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home_remote/model2_doc2vec.pkl']"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#joblib a model\n",
    "joblib.dump(model, os.path.join(HOME_DIR,'model_doc2vec.pkl'))\n",
    "joblib.dump(model2, os.path.join(HOME_DIR,'model2_doc2vec.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "fname = get_tmpfile(os.path.join(HOME_DIR,\"master_thesis/model_evaluation/my_doc2vec_model\"))\n",
    "model.save(fname)\n",
    "fname2 = get_tmpfile(os.path.join(HOME_DIR,\"master_thesis/model_evaluation/my_doc2vec_model2\"))\n",
    "model2.save(fname2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = train['text'][0:3].str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.4728922e-03,  1.1294025e-03, -2.4310261e-04, -9.6121192e-04,\n",
       "        4.7670985e-03,  1.4386200e-03, -3.0160856e-03, -4.8014042e-03,\n",
       "        1.2395275e-03,  2.8639727e-03,  4.8470020e-04,  1.5936660e-03,\n",
       "       -3.5161019e-04, -4.6375393e-05,  3.9771260e-03,  4.8645083e-03,\n",
       "       -6.5763446e-04,  8.5542200e-04,  3.2963913e-03, -1.0223204e-03,\n",
       "        1.3308584e-03, -3.0275257e-04,  3.9787532e-04, -3.4252019e-03,\n",
       "        4.0295329e-03,  4.9934722e-03, -2.5388356e-03,  3.1203639e-03,\n",
       "        3.1022662e-03,  2.1049499e-03,  2.4390297e-03, -4.6553402e-03,\n",
       "       -1.4479510e-03,  5.9771957e-04, -1.0302877e-03, -4.6756309e-03,\n",
       "       -3.4628245e-03,  4.5805462e-03,  1.5165460e-03, -1.1353013e-03,\n",
       "       -9.7143231e-04,  3.7325716e-03,  1.4615315e-03, -4.1971123e-03,\n",
       "       -9.2977466e-04, -2.8820890e-03, -2.9301124e-03, -1.8280179e-03,\n",
       "       -3.2186517e-03, -7.8540383e-04, -6.8161875e-04,  8.2067726e-04,\n",
       "        3.9903154e-03,  1.3948452e-03,  4.7700340e-03,  1.1494773e-03,\n",
       "        4.2567230e-03, -2.2238069e-03,  3.5481434e-03, -2.8138160e-04,\n",
       "       -1.4928820e-03,  4.6760468e-03, -3.0061046e-03,  3.1790298e-03,\n",
       "       -1.5946251e-03, -4.6265083e-03, -5.3862273e-04,  3.1345307e-03,\n",
       "        1.4858037e-03, -8.2761348e-05,  2.0986318e-03,  4.9346092e-04,\n",
       "       -4.2524375e-03,  2.4418521e-03, -3.6324388e-03,  5.8381556e-05,\n",
       "        4.1345907e-03,  2.7707266e-03, -3.4056532e-03,  1.8296666e-03,\n",
       "       -4.1672522e-03, -2.8554338e-03,  1.2312937e-03,  1.4280170e-03,\n",
       "       -1.1389068e-03,  2.2863590e-03,  3.6783509e-03, -4.6907896e-03,\n",
       "       -1.4834574e-03, -4.5069489e-03,  1.2839037e-03, -3.3452129e-03,\n",
       "        4.8444998e-03, -2.2095719e-03,  3.8788808e-03,  2.0167148e-03,\n",
       "        3.4807974e-03, -3.6942705e-03, -8.3193067e-04,  4.1481853e-05],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.infer_vector(read_corpus(tt, tokens_only=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec = joblib.load(os.path.join(HOME_DIR, \"lg2.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "conventional_model = LogisticRegression(solver='liblinear', C = 32, class_weight={0: 0.2, 1: 0.8}, fit_intercept=True, penalty='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=32, class_weight={0: 0.2, 1: 0.8}, solver='liblinear')"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conventional_model.fit(X_doc2vec, y_doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = conventional_model.predict(X_doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Actual  Predicted\n",
       "67        1          0\n",
       "234       0          1\n",
       "241       1          0\n",
       "252       1          0\n",
       "330       0          1"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compare y_pred2 and y_doc2vec\n",
    "con = pd.DataFrame({'Actual': y_doc2vec, 'Predicted': y_pred2})\n",
    "#misclassified\n",
    "con[con['Actual'] != con['Predicted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home_remote/conventional_model.pkl']"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save model conventional_model\n",
    "joblib.dump(conventional_model, os.path.join(HOME_DIR,'conventional_model.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "conventional_model_loaded = joblib.load(os.path.join(HOME_DIR, \"conventional_model.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conventional_model_loaded.predict(X_doc2vec[67].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-test split X_doc2vec, y_doc2vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_doc2vec, y_doc2vec, test_size=0.2, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 18:24:52.625036: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-26 18:24:53.220730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10794 MB memory:  -> device: 0, name: Tesla K80, pci bus id: 0000:8d:00.0, compute capability: 3.7\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "add() got an unexpected keyword argument 'input_shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/thi.tra.my.nguyen/master_thesis/models.ipynb Cell 48\u001b[0m line \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdm2/home/thi.tra.my.nguyen/master_thesis/models.ipynb#Y104sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Define the LSTM model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdm2/home/thi.tra.my.nguyen/master_thesis/models.ipynb#Y104sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m Sequential()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdm2/home/thi.tra.my.nguyen/master_thesis/models.ipynb#Y104sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m model\u001b[39m.\u001b[39;49madd(input_shape\u001b[39m=\u001b[39;49m(\u001b[39m10\u001b[39;49m, \u001b[39m1\u001b[39;49m))  \u001b[39m# Embedding layer for word embeddings\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdm2/home/thi.tra.my.nguyen/master_thesis/models.ipynb#Y104sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m model\u001b[39m.\u001b[39madd(LSTM(\u001b[39m128\u001b[39m))  \u001b[39m# LSTM layer with 128 units\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdm2/home/thi.tra.my.nguyen/master_thesis/models.ipynb#Y104sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m model\u001b[39m.\u001b[39madd(Dense(\u001b[39m1\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msigmoid\u001b[39m\u001b[39m'\u001b[39m))  \u001b[39m# Output layer for binary classification with sigmoid activation\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/tracking/base.py:629\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 629\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    630\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "\u001b[0;31mTypeError\u001b[0m: add() got an unexpected keyword argument 'input_shape'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(input_shape=(10, 1))  # Embedding layer for word embeddings\n",
    "model.add(LSTM(128))  # LSTM layer with 128 units\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification with sigmoid activation\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Predict probabilities on new data\n",
    "X_new_data = np.random.rand(10, 10, 100)  # Replace with your new data\n",
    "predicted_probabilities = model.predict(X_new_data)\n",
    "print(predicted_probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/thi.tra.my.nguyen/master_thesis/models.ipynb Cell 49\u001b[0m line \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdm2/home/thi.tra.my.nguyen/master_thesis/models.ipynb#Y111sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model2\u001b[39m.\u001b[39;49minfer_vector(train_corpus[\u001b[39m0\u001b[39;49m])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gensim/models/doc2vec.py:628\u001b[0m, in \u001b[0;36mDoc2Vec.infer_vector\u001b[0;34m(self, doc_words, alpha, min_alpha, epochs)\u001b[0m\n\u001b[1;32m    625\u001b[0m min_alpha \u001b[39m=\u001b[39m min_alpha \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_alpha\n\u001b[1;32m    626\u001b[0m epochs \u001b[39m=\u001b[39m epochs \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs\n\u001b[0;32m--> 628\u001b[0m doctag_vectors \u001b[39m=\u001b[39m pseudorandom_weak_vector(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdv\u001b[39m.\u001b[39mvector_size, seed_string\u001b[39m=\u001b[39m\u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(doc_words))\n\u001b[1;32m    629\u001b[0m doctag_vectors \u001b[39m=\u001b[39m doctag_vectors\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdv\u001b[39m.\u001b[39mvector_size)\n\u001b[1;32m    631\u001b[0m doctags_lockf \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones(\u001b[39m1\u001b[39m, dtype\u001b[39m=\u001b[39mREAL)\n",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"
     ]
    }
   ],
   "source": [
    "model2.infer_vector(train_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg2 = joblib.load(os.path.join(HOME_DIR, \"lg2.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9857112907136578"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg2.predict_proba(X_doc2vec[8].reshape(1, -1))[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
