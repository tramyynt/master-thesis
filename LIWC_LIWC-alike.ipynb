{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.0/250.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.1.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_DIR = \"/home_remote\"\n",
    "HOME = \"/home/thi.tra.my.nguyen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from liwc import Liwc\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LIWC dictionary\n",
    "liwc = Liwc(os.path.join(HOME_DIR, \"master_thesis/LIWC2007_English100131.dic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "%store -r df\n",
    "%store -r df_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all text of same user into one string\n",
    "text_per_user_pos = df.groupby('User')['Text'].apply(' '.join).reset_index()\n",
    "text_per_user_neg = df_neg.groupby('User')['Text'].apply(' '.join).reset_index()\n",
    "#add label\n",
    "text_per_user_pos['label'] = 1\n",
    "text_per_user_neg['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BACKUP\n",
    "#combine Text and Title\n",
    "df['Text_Title'] = df['Title'] + '.' + df['Text']\n",
    "df_neg['Text_Title'] = df_neg['Title'] + '.' + df_neg['Text']\n",
    "#combine all text of same user into one string\n",
    "text_per_user_pos = df.groupby('User')['Text_Title'].apply(' '.join).reset_index()\n",
    "text_per_user_neg = df_neg.groupby('User')['Text_Title'].apply(' '.join).reset_index()\n",
    "#add label\n",
    "text_per_user_pos['label'] = 1\n",
    "text_per_user_neg['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input first user of positive to liwc\n",
    "temp = liwc.parse(text_per_user_pos['Text'][0].split(' '))\n",
    "#sort temp descending\n",
    "temp = sorted(temp.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input text to LIWC\n",
    "liwc_pos = [liwc.parse(text.split(' ')) for text in text_per_user_pos['Text']]\n",
    "liwc_neg = [liwc.parse(text.split(' ')) for text in text_per_user_neg['Text']]\n",
    "#then buid a dictionary with key as user, value as a list of LIWC features\n",
    "liwc_pos_dict = {}\n",
    "liwc_neg_dict = {}\n",
    "for i in range(len(liwc_pos)):\n",
    "    liwc_pos_dict[text_per_user_pos['User'][i]] = liwc_pos[i]\n",
    "    liwc_neg_dict[text_per_user_neg['User'][i]] = liwc_neg[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BACKUP\n",
    "# input text to LIWC\n",
    "liwc_pos = [liwc.parse(text.split(' ')) for text in text_per_user_pos['Text_Title']]\n",
    "liwc_neg = [liwc.parse(text.split(' ')) for text in text_per_user_neg['Text_Title']]\n",
    "#then buid a dictionary with key as user, value as a list of LIWC features\n",
    "liwc_pos_dict = {}\n",
    "liwc_neg_dict = {}\n",
    "for i in range(len(liwc_pos)):\n",
    "    liwc_pos_dict[text_per_user_pos['User'][i]] = liwc_pos[i]\n",
    "for i in range(len(liwc_neg)):\n",
    "    liwc_neg_dict[text_per_user_neg['User'][i]] = liwc_neg[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate percentage of each LIWC feature per user\n",
    "liwc_pos_percent = {}\n",
    "liwc_neg_percent = {}\n",
    "for key, value in liwc_pos_dict.items():\n",
    "    liwc_pos_percent[key] = {k: v / sum(value.values()) for k, v in value.items()}\n",
    "for key, value in liwc_neg_dict.items():\n",
    "    liwc_neg_percent[key] = {k: v / sum(value.values()) for k, v in value.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation of LIWC features with label\n",
    "liwc_pos_percent_df = pd.DataFrame(liwc_pos_percent).T\n",
    "liwc_neg_percent_df = pd.DataFrame(liwc_neg_percent).T\n",
    "liwc_pos_percent_df['label'] = 1\n",
    "liwc_neg_percent_df['label'] = 0\n",
    "liwc_percent_df = pd.concat([liwc_pos_percent_df, liwc_neg_percent_df])\n",
    "liwc_percent_df = liwc_percent_df.fillna(0)\n",
    "corr = liwc_percent_df.corr()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "i          0.360488\n",
       "anx        0.358541\n",
       "health     0.348380\n",
       "sad        0.345127\n",
       "affect     0.321898\n",
       "negemo     0.320290\n",
       "feel       0.278152\n",
       "ppron      0.272873\n",
       "adverb     0.249147\n",
       "pronoun    0.248551\n",
       "insight    0.213088\n",
       "excl       0.205903\n",
       "discrep    0.204839\n",
       "cogmech    0.203785\n",
       "filler     0.198220\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#choose the top 15 most correlated features to label\n",
    "corr_label = corr['label'].sort_values(ascending=False)\n",
    "corr_label = corr_label[1:16]\n",
    "corr_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIWCalike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home_remote/dic_avg100_annotated_official.xlsx', 'rb') as f:\n",
    "    liwc2 = pd.read_excel(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "liwc2['Count'] = liwc2['Term'].str.split().str.len()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Category</th>\n",
       "      <th>Term</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Pronouns</td>\n",
       "      <td>['both', 'somebody', 'naught', 'whoever', 'eve...</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>I</td>\n",
       "      <td>['mine', 'my', 'me', 'I', 'myself']</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>We</td>\n",
       "      <td>['our', 'us', 'we', 'ours', 'ourselves']</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>You</td>\n",
       "      <td>['yourself', 'thine', 'thyself', 'your', 'yon'...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Shehe</td>\n",
       "      <td>['her', 'hers', 'him', 'she', 'his', 'herself'...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>They</td>\n",
       "      <td>['their', 'theirs', 'them', 'they', 'themselve...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>ipron</td>\n",
       "      <td>['both', 'everything', 'somebody', 'they', 'fe...</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Articles</td>\n",
       "      <td>['an', 'the', 'a']</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>auxverb</td>\n",
       "      <td>['should', 'is', 'could', \"amn't\", 'will', 'mi...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Past tense</td>\n",
       "      <td>['had', 'were', 'went', 'was', 'walked', 'came...</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Present tense</td>\n",
       "      <td>['does', 'hear', 'do', \"doesn't\", 'know', 'hea...</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Future tense</td>\n",
       "      <td>['will', 'gonna', \"they'll\", \"it'll\", \"won't\",...</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Adverbs</td>\n",
       "      <td>['everywhere', 'slowly', 'mostly', 'quickly', ...</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Prepositions</td>\n",
       "      <td>[ 'above', 'below', 'in', 'on', 'whereas', 'wh...</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6.0</td>\n",
       "      <td>Negations</td>\n",
       "      <td>['not', 'no', 'never', 'nothing', 'none', 'but...</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7.0</td>\n",
       "      <td>Numbers</td>\n",
       "      <td>['million', 'thousand', 'hundred', 'ten', 'fiv...</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8.0</td>\n",
       "      <td>Social Processes</td>\n",
       "      <td>['talk', 'friends', 'friend', 'us', 'me', 'him...</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9.0</td>\n",
       "      <td>Friends</td>\n",
       "      <td>['buddy', 'coworker', 'co-worker', 'friend', '...</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10.0</td>\n",
       "      <td>Family</td>\n",
       "      <td>['sister', 'brother', 'cousin', 'dad', 'mom', ...</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>11.0</td>\n",
       "      <td>Humans</td>\n",
       "      <td>['woman', 'girl', 'girls', 'men', 'man', 'boy'...</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>12.0</td>\n",
       "      <td>Affective Processes</td>\n",
       "      <td>['startled', 'sickened', 'offended', 'repulsiv...</td>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>13.0</td>\n",
       "      <td>Positive Emotions</td>\n",
       "      <td>['fine', 'compassion', 'harmony', 'mirthful', ...</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>14.0</td>\n",
       "      <td>Negative Emotions</td>\n",
       "      <td>['hurt', 'enemie', 'ugly', 'displeasure', 'hum...</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>15.0</td>\n",
       "      <td>Anxiety</td>\n",
       "      <td>['nervous', 'anxious', 'tense', 'scared', 'afr...</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>16.0</td>\n",
       "      <td>Anger</td>\n",
       "      <td>['pissed', 'kill', 'hate', 'killing', 'mad', '...</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>17.0</td>\n",
       "      <td>Sadness</td>\n",
       "      <td>['cry', 'sad', 'grief', 'crying', 'sadness', '...</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>18.0</td>\n",
       "      <td>Cognitive Processes</td>\n",
       "      <td>['cause', 'know', 'do', 'mean', 'think', 'unde...</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>19.0</td>\n",
       "      <td>Insight</td>\n",
       "      <td>['think', 'consider', 'know', 'say', 'assume',...</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>20.0</td>\n",
       "      <td>Causation</td>\n",
       "      <td>['effect', 'because', 'hence', 'however', 'tha...</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>21.0</td>\n",
       "      <td>Discrepancy</td>\n",
       "      <td>['would', 'could', 'should', 'might', \"wouldn'...</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>22.0</td>\n",
       "      <td>Tentative</td>\n",
       "      <td>['maybe', 'perhaps', 'guess', 'suppose', 'prob...</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>23.0</td>\n",
       "      <td>Certainty</td>\n",
       "      <td>['never', 'always', 'rarely', 'ever', 'often',...</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>24.0</td>\n",
       "      <td>Inhibition</td>\n",
       "      <td>['block', 'constrain', 'blocks', 'restrict', '...</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>25.0</td>\n",
       "      <td>Inclusive</td>\n",
       "      <td>['with', 'and', 'etc', 'have', 'all', 'includi...</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>26.0</td>\n",
       "      <td>Exclusive</td>\n",
       "      <td>['but', 'without',  'however', 'though', 'exce...</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>27.0</td>\n",
       "      <td>Perceptual Processes</td>\n",
       "      <td>['listen', 'see', 'touch', 'hear', 'talk', 'te...</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>28.0</td>\n",
       "      <td>Seeing</td>\n",
       "      <td>['look', 'see', 'view', 'looked', 'seeing', 's...</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>29.0</td>\n",
       "      <td>Hearing</td>\n",
       "      <td>['hear', 'listen', 'sound', 'heard', 'listenin...</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>30.0</td>\n",
       "      <td>Feeling</td>\n",
       "      <td>['felt', 'touch', 'hold', 'feel', 'feels', 'fe...</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>31.0</td>\n",
       "      <td>Biological Processes</td>\n",
       "      <td>['blood', 'pain', 'eat', 'stomach', 'eating', ...</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>32.0</td>\n",
       "      <td>Body</td>\n",
       "      <td>['heart', 'ache', 'aching', 'achy', 'aches', '...</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Health</td>\n",
       "      <td>['psychology', 'optometrist', 'homeopathic', '...</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>33.0</td>\n",
       "      <td>Sexuality</td>\n",
       "      <td>['horny', 'incest', 'kinky', 'incestual', 'inc...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>34.0</td>\n",
       "      <td>Relativity</td>\n",
       "      <td>['exit', 'area', 'bend', 'exits', 'stop', 'mov...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>35.0</td>\n",
       "      <td>Motion</td>\n",
       "      <td>['go', 'move', 'sit', 'moving', 'walking', 'st...</td>\n",
       "      <td>298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>36.0</td>\n",
       "      <td>Space</td>\n",
       "      <td>['thin',  'thick', 'small', 'large', 'uneven',...</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>37.0</td>\n",
       "      <td>Time</td>\n",
       "      <td>['hour', 'day', 'hours', 'week', 'afternoon', ...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>38.0</td>\n",
       "      <td>Work</td>\n",
       "      <td>['class', 'work', 'boss', 'classes', 'bosses',...</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>39.0</td>\n",
       "      <td>Achievement</td>\n",
       "      <td>['goal', 'win', 'goals', 'winning', 'try', 'su...</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>40.0</td>\n",
       "      <td>Leisure</td>\n",
       "      <td>['music', 'house', 'apartment', 'houses', 'bed...</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>41.0</td>\n",
       "      <td>Home</td>\n",
       "      <td>['house', 'kitchen', 'lawn', 'porch', 'apartme...</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>42.0</td>\n",
       "      <td>Money</td>\n",
       "      <td>['cash', 'money', 'owe', 'audit', 'pay', 'owed...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>43.0</td>\n",
       "      <td>Religion</td>\n",
       "      <td>['church', 'mosque', 'congregation', 'synagogu...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>44.0</td>\n",
       "      <td>Death</td>\n",
       "      <td>['bury', 'kill', 'die', 'coffin', 'decapitate'...</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>45.0</td>\n",
       "      <td>Assent</td>\n",
       "      <td>['agree', 'yes', 'think', 'say', 'agreed', 'ce...</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>46.0</td>\n",
       "      <td>Nonfluencies</td>\n",
       "      <td>['uh', 'um', 'uhh', 'uhhh', 'umm', 'ummm', 'uh...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>47.0</td>\n",
       "      <td>Fillers</td>\n",
       "      <td>['blah', 'blahblahblah', 'bla', 'yada', 'blahb...</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0              Category  \\\n",
       "0          0.0              Pronouns   \n",
       "1          NaN                     I   \n",
       "2          NaN                    We   \n",
       "3          NaN                   You   \n",
       "4          NaN                 Shehe   \n",
       "5          NaN                  They   \n",
       "6          NaN                 ipron   \n",
       "7          1.0              Articles   \n",
       "8          NaN               auxverb   \n",
       "9          2.0            Past tense   \n",
       "10         3.0         Present tense   \n",
       "11         4.0          Future tense   \n",
       "12         NaN               Adverbs   \n",
       "13         5.0          Prepositions   \n",
       "14         6.0             Negations   \n",
       "15         7.0               Numbers   \n",
       "16         8.0      Social Processes   \n",
       "17         9.0               Friends   \n",
       "18        10.0                Family   \n",
       "19        11.0                Humans   \n",
       "20        12.0   Affective Processes   \n",
       "21        13.0     Positive Emotions   \n",
       "22        14.0     Negative Emotions   \n",
       "23        15.0               Anxiety   \n",
       "24        16.0                 Anger   \n",
       "25        17.0               Sadness   \n",
       "26        18.0   Cognitive Processes   \n",
       "27        19.0               Insight   \n",
       "28        20.0             Causation   \n",
       "29        21.0           Discrepancy   \n",
       "30        22.0             Tentative   \n",
       "31        23.0             Certainty   \n",
       "32        24.0            Inhibition   \n",
       "33        25.0             Inclusive   \n",
       "34        26.0             Exclusive   \n",
       "35        27.0  Perceptual Processes   \n",
       "36        28.0                Seeing   \n",
       "37        29.0               Hearing   \n",
       "38        30.0               Feeling   \n",
       "39        31.0  Biological Processes   \n",
       "40        32.0                  Body   \n",
       "41         NaN                Health   \n",
       "42        33.0             Sexuality   \n",
       "43        34.0            Relativity   \n",
       "44        35.0                Motion   \n",
       "45        36.0                 Space   \n",
       "46        37.0                  Time   \n",
       "47        38.0                  Work   \n",
       "48        39.0           Achievement   \n",
       "49        40.0               Leisure   \n",
       "50        41.0                  Home   \n",
       "51        42.0                 Money   \n",
       "52        43.0              Religion   \n",
       "53        44.0                 Death   \n",
       "54        45.0                Assent   \n",
       "55        46.0          Nonfluencies   \n",
       "56        47.0               Fillers   \n",
       "\n",
       "                                                 Term  Count  \n",
       "0   ['both', 'somebody', 'naught', 'whoever', 'eve...    124  \n",
       "1                 ['mine', 'my', 'me', 'I', 'myself']      5  \n",
       "2            ['our', 'us', 'we', 'ours', 'ourselves']      5  \n",
       "3   ['yourself', 'thine', 'thyself', 'your', 'yon'...     13  \n",
       "4   ['her', 'hers', 'him', 'she', 'his', 'herself'...     11  \n",
       "5   ['their', 'theirs', 'them', 'they', 'themselve...      7  \n",
       "6   ['both', 'everything', 'somebody', 'they', 'fe...     48  \n",
       "7                                  ['an', 'the', 'a']      3  \n",
       "8   ['should', 'is', 'could', \"amn't\", 'will', 'mi...     29  \n",
       "9   ['had', 'were', 'went', 'was', 'walked', 'came...    158  \n",
       "10  ['does', 'hear', 'do', \"doesn't\", 'know', 'hea...    177  \n",
       "11  ['will', 'gonna', \"they'll\", \"it'll\", \"won't\",...     38  \n",
       "12  ['everywhere', 'slowly', 'mostly', 'quickly', ...     54  \n",
       "13  [ 'above', 'below', 'in', 'on', 'whereas', 'wh...     63  \n",
       "14  ['not', 'no', 'never', 'nothing', 'none', 'but...    105  \n",
       "15  ['million', 'thousand', 'hundred', 'ten', 'fiv...     98  \n",
       "16  ['talk', 'friends', 'friend', 'us', 'me', 'him...    128  \n",
       "17  ['buddy', 'coworker', 'co-worker', 'friend', '...     86  \n",
       "18  ['sister', 'brother', 'cousin', 'dad', 'mom', ...     95  \n",
       "19  ['woman', 'girl', 'girls', 'men', 'man', 'boy'...     73  \n",
       "20  ['startled', 'sickened', 'offended', 'repulsiv...    222  \n",
       "21  ['fine', 'compassion', 'harmony', 'mirthful', ...    266  \n",
       "22  ['hurt', 'enemie', 'ugly', 'displeasure', 'hum...    266  \n",
       "23  ['nervous', 'anxious', 'tense', 'scared', 'afr...     90  \n",
       "24  ['pissed', 'kill', 'hate', 'killing', 'mad', '...     98  \n",
       "25  ['cry', 'sad', 'grief', 'crying', 'sadness', '...     99  \n",
       "26  ['cause', 'know', 'do', 'mean', 'think', 'unde...     59  \n",
       "27  ['think', 'consider', 'know', 'say', 'assume',...    173  \n",
       "28  ['effect', 'because', 'hence', 'however', 'tha...     83  \n",
       "29  ['would', 'could', 'should', 'might', \"wouldn'...     66  \n",
       "30  ['maybe', 'perhaps', 'guess', 'suppose', 'prob...    118  \n",
       "31  ['never', 'always', 'rarely', 'ever', 'often',...    110  \n",
       "32  ['block', 'constrain', 'blocks', 'restrict', '...     89  \n",
       "33  ['with', 'and', 'etc', 'have', 'all', 'includi...     46  \n",
       "34  ['but', 'without',  'however', 'though', 'exce...     71  \n",
       "35  ['listen', 'see', 'touch', 'hear', 'talk', 'te...     74  \n",
       "36  ['look', 'see', 'view', 'looked', 'seeing', 's...     98  \n",
       "37  ['hear', 'listen', 'sound', 'heard', 'listenin...     95  \n",
       "38  ['felt', 'touch', 'hold', 'feel', 'feels', 'fe...    190  \n",
       "39  ['blood', 'pain', 'eat', 'stomach', 'eating', ...    112  \n",
       "40  ['heart', 'ache', 'aching', 'achy', 'aches', '...    124  \n",
       "41  ['psychology', 'optometrist', 'homeopathic', '...    199  \n",
       "42  ['horny', 'incest', 'kinky', 'incestual', 'inc...    100  \n",
       "43  ['exit', 'area', 'bend', 'exits', 'stop', 'mov...    100  \n",
       "44  ['go', 'move', 'sit', 'moving', 'walking', 'st...    298  \n",
       "45  ['thin',  'thick', 'small', 'large', 'uneven',...     73  \n",
       "46  ['hour', 'day', 'hours', 'week', 'afternoon', ...    100  \n",
       "47  ['class', 'work', 'boss', 'classes', 'bosses',...    166  \n",
       "48  ['goal', 'win', 'goals', 'winning', 'try', 'su...     98  \n",
       "49  ['music', 'house', 'apartment', 'houses', 'bed...    122  \n",
       "50  ['house', 'kitchen', 'lawn', 'porch', 'apartme...    113  \n",
       "51  ['cash', 'money', 'owe', 'audit', 'pay', 'owed...    100  \n",
       "52  ['church', 'mosque', 'congregation', 'synagogu...    100  \n",
       "53  ['bury', 'kill', 'die', 'coffin', 'decapitate'...     98  \n",
       "54  ['agree', 'yes', 'think', 'say', 'agreed', 'ce...     56  \n",
       "55  ['uh', 'um', 'uhh', 'uhhh', 'umm', 'ummm', 'uh...    100  \n",
       "56  ['blah', 'blahblahblah', 'bla', 'yada', 'blahb...    100  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liwc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5817"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def main(input_, dic):\n",
    "    dic['Terms'] = dic['Term'].apply(lambda x: ast.literal_eval(x))\n",
    "    result = dict(zip(dic['Category'], dic['Terms']))\n",
    "    lexicon = parse_dict(result)\n",
    "    parse, category_names = load_token_parser(result)\n",
    "    \n",
    "    input_tokens = tokenize(input_.lower())\n",
    "    counts = Counter(category for token in input_tokens for category in parse(token))\n",
    "    \n",
    "    a = pd.DataFrame.from_dict(dict(counts),orient = 'index').reset_index()\n",
    "    a= a.rename(columns={\"index\": \"Category\", 0: \"Count\"})\n",
    "    a['Percentage(%)']= round(a['Count']/len(input_.split(' '))*100,2)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
