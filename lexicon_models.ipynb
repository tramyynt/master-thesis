{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_DIR = \"/home_remote\"\n",
    "HOME = \"/home/thi.tra.my.nguyen\"\n",
    "\n",
    "from liwc import Liwc\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize, ngrams\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_features_name_without_Length ={'liwc': ['i', 'friend', 'sad', 'family', 'feel', 'health',\n",
    "       'sexual', 'anx', 'body', 'bio', 'ppron', 'filler', 'shehe', 'adverb',\n",
    "       'swear', 'humans', 'excl', 'assent', 'discrep', 'you', 'pronoun',\n",
    "       'negemo', 'past'],\n",
    "                        'liwc_alike': ['Anxiety', 'I', 'Sadness', 'Affective Processes',\n",
    "       'Sexuality', 'Family', 'Friends', 'Fillers', 'Health', 'Feeling',\n",
    "       'Humans', 'Biological Processes', 'Time', 'Body', 'Negative Emotions',\n",
    "       'Social Processes', 'Perceptual Processes', 'Insight',\n",
    "       'Cognitive Processes', 'Motion', 'Positive Emotions', 'Tentative',\n",
    "       'Ppronouns']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives_df_path = os.path.join(HOME_DIR, \"positive_df.pkl\")\n",
    "negatives_df_path = os.path.join(HOME_DIR, \"negative_df.pkl\")\n",
    "\n",
    "positives = pd.read_pickle(positives_df_path)\n",
    "negatives = pd.read_pickle(negatives_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_liwc_input(df, label):\n",
    "  \"\"\"\n",
    "  params: df - The positive/negative dataframe loaded from pickle\n",
    "    The df is expected to has these columns \"Title\", \"Date\", \"Text\", \"SubjectId\"\n",
    "  params: label - The label need to be assigned to result dataframe\n",
    "\n",
    "  returns: A dataframe contains \"SubjectId\", \"AverageLength\", \"Text\", \"NumOfWritings\", \"Title\"\n",
    "  \"\"\"\n",
    "  subject_id_list = df.loc[:, \"TrainSubjectId\"].unique()\n",
    "  df[\"Token\"] = df[\"Text\"].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "  df['text'] = df['Text']+ df['Title']\n",
    "\n",
    "  grouped_by_subject_id = df.groupby('TrainSubjectId')\n",
    "\n",
    "  # calculate average token length for each user\n",
    "  average_length_df = grouped_by_subject_id['Token'].apply(lambda token_series: sum(len(token) for token in token_series) / len(token_series)).reset_index()\n",
    "  average_length_df.rename(columns={'Token': 'AverageLength'}, inplace=True)\n",
    "  #print(average_length_df.head())\n",
    "\n",
    "  # join all writings of single user into single corpus\n",
    "  joined_text_df = grouped_by_subject_id['text'].apply(' '.join).reset_index()\n",
    "\n",
    "  # calculate number of writings for each user\n",
    "  number_of_writings_df = grouped_by_subject_id['Text'].apply(lambda x: len(x)).reset_index()\n",
    "  number_of_writings_df.rename(columns={'Text': 'NumOfWritings'}, inplace=True)\n",
    "\n",
    "  result_df = average_length_df.merge(joined_text_df, on=\"TrainSubjectId\")\n",
    "  result_df = result_df.merge(number_of_writings_df, on=\"TrainSubjectId\")\n",
    "  result_df[\"Label\"] = label\n",
    "\n",
    "  return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct liwc input for positive and negative dataframe\n",
    "input_positives = construct_liwc_input(positives, 1)\n",
    "input_negatives = construct_liwc_input(negatives, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate two dataframe and shuffle\n",
    "liwc_input = pd.concat([input_positives, input_negatives])\n",
    "liwc_input = liwc_input.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TrainSubjectId</th>\n",
       "      <th>AverageLength</th>\n",
       "      <th>text</th>\n",
       "      <th>NumOfWritings</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_subject6828</td>\n",
       "      <td>20.122711</td>\n",
       "      <td>Have you seen Bloodline? That first season is...</td>\n",
       "      <td>1092</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_subject8603</td>\n",
       "      <td>21.882812</td>\n",
       "      <td>I met KC Green about 5 years ago and asked ...</td>\n",
       "      <td>256</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_subject5173</td>\n",
       "      <td>28.108974</td>\n",
       "      <td>Thanks for sharing and I can imagine that! Es...</td>\n",
       "      <td>156</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_subject1637</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>Like sliggoo? Common guys it's not fat green ...</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_subject2006</td>\n",
       "      <td>67.378981</td>\n",
       "      <td>I'm in enthusiastic agreement with you      O...</td>\n",
       "      <td>314</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>train_subject634</td>\n",
       "      <td>35.316327</td>\n",
       "      <td>He got 2nd place in Evo 2012 too :(  Gamerbee...</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>train_subject5276</td>\n",
       "      <td>21.626087</td>\n",
       "      <td>also i need a name for the disputed territory...</td>\n",
       "      <td>230</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>train_subject138</td>\n",
       "      <td>32.602005</td>\n",
       "      <td>The boozing starts from 7am. Though large a...</td>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>train_subject3364</td>\n",
       "      <td>37.688525</td>\n",
       "      <td>Cat.       Thank you for watching out for the...</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>train_subject1485</td>\n",
       "      <td>28.273637</td>\n",
       "      <td>Suspend Hope Solo Immediately     Chronicle...</td>\n",
       "      <td>1999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>486 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        TrainSubjectId  AverageLength  \\\n",
       "0    train_subject6828      20.122711   \n",
       "1    train_subject8603      21.882812   \n",
       "2    train_subject5173      28.108974   \n",
       "3    train_subject1637      32.000000   \n",
       "4    train_subject2006      67.378981   \n",
       "..                 ...            ...   \n",
       "481   train_subject634      35.316327   \n",
       "482  train_subject5276      21.626087   \n",
       "483   train_subject138      32.602005   \n",
       "484  train_subject3364      37.688525   \n",
       "485  train_subject1485      28.273637   \n",
       "\n",
       "                                                  text  NumOfWritings  Label  \n",
       "0     Have you seen Bloodline? That first season is...           1092      0  \n",
       "1       I met KC Green about 5 years ago and asked ...            256      0  \n",
       "2     Thanks for sharing and I can imagine that! Es...            156      0  \n",
       "3     Like sliggoo? Common guys it's not fat green ...             12      1  \n",
       "4     I'm in enthusiastic agreement with you      O...            314      0  \n",
       "..                                                 ...            ...    ...  \n",
       "481   He got 2nd place in Evo 2012 too :(  Gamerbee...             98      0  \n",
       "482   also i need a name for the disputed territory...            230      0  \n",
       "483     The boozing starts from 7am. Though large a...           1995      0  \n",
       "484   Cat.       Thank you for watching out for the...            122      1  \n",
       "485     Suspend Hope Solo Immediately     Chronicle...           1999      0  \n",
       "\n",
       "[486 rows x 5 columns]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liwc_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIWC Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LIWC dictionary\n",
    "liwc = Liwc(os.path.join(HOME_DIR, \"master_thesis/LIWC2007_English100131.dic\"))\n",
    "input = [liwc.parse(word_tokenize(text)) for text in liwc_input['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add AverageLength and NumOfWritings to the vector\n",
    "def add_to_counter(counter, key, value):\n",
    "    counter[key] = value\n",
    "    return counter\n",
    "\n",
    "# Get features\n",
    "def get_features(df, output):\n",
    "    df['vector'] = output\n",
    "    average_length = df['AverageLength']\n",
    "    num_of_writings = df['NumOfWritings']\n",
    "    vector = df['vector']\n",
    "    for i in range(len(vector)):\n",
    "        vector[i] = add_to_counter(vector[i], \"AverageLength\", average_length[i])\n",
    "        vector[i] = add_to_counter(vector[i], \"NumOfWritings\", num_of_writings[i])\n",
    "    df['vector_added'] = vector\n",
    "    vector_df = pd.DataFrame(df['vector_added'].tolist(), index=df.index)\n",
    "    vector_df_norm = (vector_df - vector_df.min()) / (vector_df.max() - vector_df.min())\n",
    "    vector_df_norm['Label'] = df['Label']\n",
    "    vector_df_norm['TrainSubjectId'] = df['TrainSubjectId']\n",
    "    vector_df_norm = vector_df_norm.fillna(0)\n",
    "    corr = vector_df_norm.corr()\n",
    "    corr_label = corr['Label'].sort_values(ascending=False)\n",
    "    relevant_features = corr_label[1:25]\n",
    "    relevant_features_name = relevant_features.index.values\n",
    "    X = vector_df_norm[relevant_features_name]\n",
    "    y = vector_df_norm['Label']\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_100870/3541103378.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  vector[i] = add_to_counter(vector[i], \"AverageLength\", average_length[i])\n",
      "/tmp/ipykernel_100870/3541103378.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  vector[i] = add_to_counter(vector[i], \"NumOfWritings\", num_of_writings[i])\n"
     ]
    }
   ],
   "source": [
    "X_liwc, y_liwc = get_features(liwc_input, op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIWC-alike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run /home_remote/master_thesis/model_evaluation/liwc_alike.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_alike_output = [main(text, result) for text in liwc_input['text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_100870/3541103378.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  vector[i] = add_to_counter(vector[i], \"AverageLength\", average_length[i])\n",
      "/tmp/ipykernel_100870/3541103378.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  vector[i] = add_to_counter(vector[i], \"NumOfWritings\", num_of_writings[i])\n"
     ]
    }
   ],
   "source": [
    "X_liwc_alike, y_liwc_alike = get_features(liwc_input, liwc_alike_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, y):\n",
    "\n",
    "    w = [1, 2**1, 2**2, 2**3, 2**4, 2**5, 2**6,2**7, 2**8]\n",
    "    weight = [{0: 1/(1+x),  1: x/(1+x)} for x in w]\n",
    "    C = [2**-6, 2**-5, 2**-4, 2**-3, 2**-2, 2**-1, 1, 2**1, 2**2, 2**3, 2**4, 2**5, 2**6]\n",
    "    # define grid search\n",
    "    hyperparam_grid = {\"class_weight\": weight\n",
    "                    ,\"C\": C\n",
    "                    ,\"fit_intercept\": [True, False]  }\n",
    "    # define evaluation procedure\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "    # define grid search\n",
    "    model_test = LogisticRegression(solver='liblinear', penalty='l1')\n",
    "    grid = GridSearchCV(estimator=model_test, param_grid=hyperparam_grid, cv=cv, scoring='f1')\n",
    "    grid_result = grid.fit(X, y)\n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "    #build a model with the best parameters, fix class_weight = (0.2, 0.8)\n",
    "\n",
    "    model = LogisticRegression(**grid_result.best_params_)\n",
    "    model.fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.334819 using {'C': 16, 'class_weight': {0: 0.2, 1: 0.8}, 'fit_intercept': True}\n",
      "[0.25       0.07692308 0.         0.4375     0.28571429]\n"
     ]
    }
   ],
   "source": [
    "mod = logistic_regression(X_liwc, y_liwc)\n",
    "#cross_validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(mod, X_liwc, y_liwc, cv=5, scoring='f1')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X, y):\n",
    "\n",
    "    w = [1, 2**1, 2**2, 2**3, 2**4, 2**5, 2**6,2**7, 2**8]\n",
    "    weight = [{0: 1/(1+x),  1: x/(1+x)} for x in w]\n",
    "    #C = [2**-6, 2**-5, 2**-4, 2**-3, 2**-2, 2**-1, 1, 2**1, 2**2, 2**3, 2**4, 2**5, 2**6]\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "    # Number of features to consider at every split\n",
    "    max_features = ['auto', 'sqrt']\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "    max_depth.append(None)\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = [2, 5, 10]\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "    # Method of selecting samples for training each tree\n",
    "    bootstrap = [True, False]\n",
    "    # define grid search\n",
    "    hyperparam_grid = {\"class_weight\": weight,\n",
    "                        'n_estimators': n_estimators,\n",
    "                        'max_features': max_features,\n",
    "                        'max_depth': max_depth,\n",
    "                        'min_samples_split': min_samples_split,\n",
    "                        'min_samples_leaf': min_samples_leaf,\n",
    "                        'bootstrap': bootstrap}\n",
    "    # define evaluation procedure\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "    # define grid search\n",
    "    model_test = RandomForestClassifier()\n",
    "    grid = GridSearchCV(estimator=model_test, param_grid=hyperparam_grid, cv=cv, scoring='f1')\n",
    "    grid_result = grid.fit(X, y)\n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "    #build a model with the best parameters, fix class_weight = (0.2, 0.8)\n",
    "\n",
    "    model = RandomForestClassifier(**grid_result.best_params_)\n",
    "    model.fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get extract\n",
    "def extract_feature(df, output, type):\n",
    "    # Assign the 'output' to a new column 'vector' in the DataFrame 'df'.\n",
    "    df['vector'] = output\n",
    "    \n",
    "    # Create a new DataFrame 'vector_df' containing the 'vector' values, with the same index as 'df'.\n",
    "    vector_df = pd.DataFrame(df['vector'].tolist(), index=df.index)\n",
    "    \n",
    "    # Normalize the values in 'vector_df' by dividing each row by the sum of its values.\n",
    "    vector_df_norm = vector_df.div(vector_df.sum(axis=1), axis=0)\n",
    "\n",
    "    # Assign 'Label' and 'TrainSubjectId' columns from 'df' to 'vector_df_norm'.\n",
    "    vector_df_norm['Label'] = df['Label']\n",
    "    vector_df_norm['TrainSubjectId'] = df['TrainSubjectId']\n",
    "\n",
    "    # Fill any NaN (Not-a-Number) values with 0.\n",
    "    vector_df_norm = vector_df_norm.fillna(0)\n",
    "\n",
    "    # Create feature matrix 'X' from the relevant features based on 'type'.\n",
    "    X = vector_df_norm[relevant_features_name_without_Length[type]]\n",
    "\n",
    "    # Create the target variable 'y' from the 'Label' column in 'vector_df_norm'.\n",
    "    y = vector_df_norm['Label']\n",
    "\n",
    "    # Return the feature matrix 'X' and the target variable 'y'.\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input = liwc_input[['TrainSubjectId', 'Label', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102552/3430175793.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['vector'] = output\n"
     ]
    }
   ],
   "source": [
    "X_liwc_alike2, y_liwc_alike2 = extract_feature(data_input, liwc_alike_output, 'liwc_alike')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anxiety</th>\n",
       "      <th>I</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Affective Processes</th>\n",
       "      <th>Sexuality</th>\n",
       "      <th>Family</th>\n",
       "      <th>Friends</th>\n",
       "      <th>Fillers</th>\n",
       "      <th>Health</th>\n",
       "      <th>Feeling</th>\n",
       "      <th>...</th>\n",
       "      <th>Body</th>\n",
       "      <th>Negative Emotions</th>\n",
       "      <th>Social Processes</th>\n",
       "      <th>Perceptual Processes</th>\n",
       "      <th>Insight</th>\n",
       "      <th>Cognitive Processes</th>\n",
       "      <th>Motion</th>\n",
       "      <th>Positive Emotions</th>\n",
       "      <th>Tentative</th>\n",
       "      <th>Ppronouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000520</td>\n",
       "      <td>0.010825</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>0.001982</td>\n",
       "      <td>0.001164</td>\n",
       "      <td>0.000991</td>\n",
       "      <td>0.003691</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.001858</td>\n",
       "      <td>0.006218</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002799</td>\n",
       "      <td>0.004261</td>\n",
       "      <td>0.022022</td>\n",
       "      <td>0.012411</td>\n",
       "      <td>0.017266</td>\n",
       "      <td>0.014046</td>\n",
       "      <td>0.011395</td>\n",
       "      <td>0.016300</td>\n",
       "      <td>0.019347</td>\n",
       "      <td>0.062376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000449</td>\n",
       "      <td>0.018402</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>0.002020</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>0.013689</td>\n",
       "      <td>0.005162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002469</td>\n",
       "      <td>0.008079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003815</td>\n",
       "      <td>0.003366</td>\n",
       "      <td>0.042415</td>\n",
       "      <td>0.006732</td>\n",
       "      <td>0.006732</td>\n",
       "      <td>0.004264</td>\n",
       "      <td>0.010772</td>\n",
       "      <td>0.007630</td>\n",
       "      <td>0.010996</td>\n",
       "      <td>0.069569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.011690</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>0.003833</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>0.002491</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.004791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>0.007666</td>\n",
       "      <td>0.022614</td>\n",
       "      <td>0.017440</td>\n",
       "      <td>0.017440</td>\n",
       "      <td>0.011882</td>\n",
       "      <td>0.014565</td>\n",
       "      <td>0.016290</td>\n",
       "      <td>0.023956</td>\n",
       "      <td>0.056535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000622</td>\n",
       "      <td>0.006970</td>\n",
       "      <td>0.000373</td>\n",
       "      <td>0.001494</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>0.001245</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.001743</td>\n",
       "      <td>0.007344</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002365</td>\n",
       "      <td>0.003236</td>\n",
       "      <td>0.048419</td>\n",
       "      <td>0.011949</td>\n",
       "      <td>0.017924</td>\n",
       "      <td>0.013443</td>\n",
       "      <td>0.012945</td>\n",
       "      <td>0.017924</td>\n",
       "      <td>0.023152</td>\n",
       "      <td>0.063231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008136</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001479</td>\n",
       "      <td>0.001479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>0.005917</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>0.008876</td>\n",
       "      <td>0.025148</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.011095</td>\n",
       "      <td>0.005178</td>\n",
       "      <td>0.015533</td>\n",
       "      <td>0.017012</td>\n",
       "      <td>0.019970</td>\n",
       "      <td>0.055473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>0.000536</td>\n",
       "      <td>0.005893</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.001875</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>0.001339</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>0.003348</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001741</td>\n",
       "      <td>0.003616</td>\n",
       "      <td>0.022097</td>\n",
       "      <td>0.015669</td>\n",
       "      <td>0.025847</td>\n",
       "      <td>0.020892</td>\n",
       "      <td>0.017544</td>\n",
       "      <td>0.013928</td>\n",
       "      <td>0.028659</td>\n",
       "      <td>0.053703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.003502</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>0.000642</td>\n",
       "      <td>0.001692</td>\n",
       "      <td>0.001692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002510</td>\n",
       "      <td>0.003794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000642</td>\n",
       "      <td>0.004085</td>\n",
       "      <td>0.028247</td>\n",
       "      <td>0.013832</td>\n",
       "      <td>0.022265</td>\n",
       "      <td>0.017479</td>\n",
       "      <td>0.013190</td>\n",
       "      <td>0.007996</td>\n",
       "      <td>0.025008</td>\n",
       "      <td>0.062360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>0.000911</td>\n",
       "      <td>0.011577</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.002090</td>\n",
       "      <td>0.001608</td>\n",
       "      <td>0.001769</td>\n",
       "      <td>0.001608</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001179</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001233</td>\n",
       "      <td>0.003752</td>\n",
       "      <td>0.029800</td>\n",
       "      <td>0.017472</td>\n",
       "      <td>0.023636</td>\n",
       "      <td>0.016722</td>\n",
       "      <td>0.013935</td>\n",
       "      <td>0.017472</td>\n",
       "      <td>0.031568</td>\n",
       "      <td>0.057026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>0.000446</td>\n",
       "      <td>0.001338</td>\n",
       "      <td>0.000892</td>\n",
       "      <td>0.001338</td>\n",
       "      <td>0.001189</td>\n",
       "      <td>0.001041</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002676</td>\n",
       "      <td>0.007434</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001933</td>\n",
       "      <td>0.007880</td>\n",
       "      <td>0.020220</td>\n",
       "      <td>0.016057</td>\n",
       "      <td>0.015611</td>\n",
       "      <td>0.011002</td>\n",
       "      <td>0.018733</td>\n",
       "      <td>0.009515</td>\n",
       "      <td>0.019923</td>\n",
       "      <td>0.023937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.001748</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008718</td>\n",
       "      <td>0.003355</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>0.003804</td>\n",
       "      <td>0.017105</td>\n",
       "      <td>0.006473</td>\n",
       "      <td>0.013655</td>\n",
       "      <td>0.011175</td>\n",
       "      <td>0.009828</td>\n",
       "      <td>0.008647</td>\n",
       "      <td>0.018900</td>\n",
       "      <td>0.038107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>486 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Anxiety         I   Sadness  Affective Processes  Sexuality    Family  \\\n",
       "0    0.000520  0.010825  0.000396             0.001982   0.001164  0.000991   \n",
       "1    0.000449  0.018402  0.000449             0.002020   0.000449  0.013689   \n",
       "2    0.000192  0.011690  0.000958             0.003833   0.001342  0.001342   \n",
       "3    0.000622  0.006970  0.000373             0.001494   0.000747  0.000747   \n",
       "4    0.000000  0.008136  0.000740             0.000740   0.000000  0.001479   \n",
       "..        ...       ...       ...                  ...        ...       ...   \n",
       "481  0.000536  0.005893  0.000134             0.001875   0.000134  0.000670   \n",
       "482  0.000233  0.003502  0.000233             0.001109   0.000642  0.001692   \n",
       "483  0.000911  0.011577  0.000375             0.002090   0.001608  0.001769   \n",
       "484  0.000446  0.001338  0.000892             0.001338   0.001189  0.001041   \n",
       "485  0.000165  0.001748  0.000095             0.000945   0.000685  0.000189   \n",
       "\n",
       "      Friends   Fillers    Health   Feeling  ...      Body  Negative Emotions  \\\n",
       "0    0.003691  0.000074  0.001858  0.006218  ...  0.002799           0.004261   \n",
       "1    0.005162  0.000000  0.002469  0.008079  ...  0.003815           0.003366   \n",
       "2    0.002491  0.000000  0.001725  0.004791  ...  0.001342           0.007666   \n",
       "3    0.001245  0.000249  0.001743  0.007344  ...  0.002365           0.003236   \n",
       "4    0.001479  0.000000  0.000740  0.005917  ...  0.000740           0.008876   \n",
       "..        ...       ...       ...       ...  ...       ...                ...   \n",
       "481  0.001339  0.000000  0.001071  0.003348  ...  0.001741           0.003616   \n",
       "482  0.001692  0.000000  0.002510  0.003794  ...  0.000642           0.004085   \n",
       "483  0.001608  0.000000  0.001179  0.008200  ...  0.001233           0.003752   \n",
       "484  0.000297  0.000000  0.002676  0.007434  ...  0.001933           0.007880   \n",
       "485  0.000260  0.000000  0.008718  0.003355  ...  0.000827           0.003804   \n",
       "\n",
       "     Social Processes  Perceptual Processes   Insight  Cognitive Processes  \\\n",
       "0            0.022022              0.012411  0.017266             0.014046   \n",
       "1            0.042415              0.006732  0.006732             0.004264   \n",
       "2            0.022614              0.017440  0.017440             0.011882   \n",
       "3            0.048419              0.011949  0.017924             0.013443   \n",
       "4            0.025148              0.009615  0.011095             0.005178   \n",
       "..                ...                   ...       ...                  ...   \n",
       "481          0.022097              0.015669  0.025847             0.020892   \n",
       "482          0.028247              0.013832  0.022265             0.017479   \n",
       "483          0.029800              0.017472  0.023636             0.016722   \n",
       "484          0.020220              0.016057  0.015611             0.011002   \n",
       "485          0.017105              0.006473  0.013655             0.011175   \n",
       "\n",
       "       Motion  Positive Emotions  Tentative  Ppronouns  \n",
       "0    0.011395           0.016300   0.019347   0.062376  \n",
       "1    0.010772           0.007630   0.010996   0.069569  \n",
       "2    0.014565           0.016290   0.023956   0.056535  \n",
       "3    0.012945           0.017924   0.023152   0.063231  \n",
       "4    0.015533           0.017012   0.019970   0.055473  \n",
       "..        ...                ...        ...        ...  \n",
       "481  0.017544           0.013928   0.028659   0.053703  \n",
       "482  0.013190           0.007996   0.025008   0.062360  \n",
       "483  0.013935           0.017472   0.031568   0.057026  \n",
       "484  0.018733           0.009515   0.019923   0.023937  \n",
       "485  0.009828           0.008647   0.018900   0.038107  \n",
       "\n",
       "[486 rows x 23 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_liwc_alike2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
