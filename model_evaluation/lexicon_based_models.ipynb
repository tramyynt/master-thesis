{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_DIR = \"/home_remote\"\n",
    "HOME = \"/home/thi.tra.my.nguyen\"\n",
    "\n",
    "from liwc import Liwc\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize, ngrams\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#import logisitic_regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives_df_path = os.path.join(HOME_DIR, \"positive_df.pkl\")\n",
    "negatives_df_path = os.path.join(HOME_DIR, \"negative_df.pkl\")\n",
    "\n",
    "positives = pd.read_pickle(positives_df_path)\n",
    "negatives = pd.read_pickle(negatives_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_liwc_input(df, label):\n",
    "  \"\"\"\n",
    "  params: df - The positive/negative dataframe loaded from pickle\n",
    "    The df is expected to has these columns \"Title\", \"Date\", \"Text\", \"SubjectId\"\n",
    "  params: label - The label need to be assigned to result dataframe\n",
    "\n",
    "  returns: A dataframe contains \"SubjectId\", \"AverageLength\", \"Text\", \"NumOfWritings\", \"Title\"\n",
    "  \"\"\"\n",
    "  subject_id_list = df.loc[:, \"TrainSubjectId\"].unique()\n",
    "  df[\"Token\"] = df[\"Text\"].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "  df['text'] = df['Text']+ df['Title']\n",
    "\n",
    "  grouped_by_subject_id = df.groupby('TrainSubjectId')\n",
    "\n",
    "  # calculate average token length for each user\n",
    "  average_length_df = grouped_by_subject_id['Token'].apply(lambda token_series: sum(len(token) for token in token_series) / len(token_series)).reset_index()\n",
    "  average_length_df.rename(columns={'Token': 'AverageLength'}, inplace=True)\n",
    "  #print(average_length_df.head())\n",
    "\n",
    "  # join all writings of single user into single corpus\n",
    "  joined_text_df = grouped_by_subject_id['text'].apply(' '.join).reset_index()\n",
    "\n",
    "  # calculate number of writings for each user\n",
    "  number_of_writings_df = grouped_by_subject_id['Text'].apply(lambda x: len(x)).reset_index()\n",
    "  number_of_writings_df.rename(columns={'Text': 'NumOfWritings'}, inplace=True)\n",
    "\n",
    "  result_df = average_length_df.merge(joined_text_df, on=\"TrainSubjectId\")\n",
    "  result_df = result_df.merge(number_of_writings_df, on=\"TrainSubjectId\")\n",
    "  result_df[\"Label\"] = label\n",
    "\n",
    "  return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_positives = construct_liwc_input(positives, 1)\n",
    "input_negatives = construct_liwc_input(negatives, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_input = pd.concat([input_positives, input_negatives])\n",
    "liwc_input = liwc_input.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input = liwc_input[['TrainSubjectId', 'Label', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LIWC dictionary\n",
    "liwc = Liwc(os.path.join(HOME_DIR, \"master_thesis/LIWC2007_English100131.dic\"))\n",
    "input = [liwc.parse(word_tokenize(text)) for text in data_input['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LIWC-alike dictionary\n",
    "%run /home_remote/master_thesis/model_evaluation/liwc_alike.py\n",
    "liwc_alike_output = [main(text, result) for text in data_input['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract top 15 features based on percentage. \n",
    "def get_features_top15(df, output):\n",
    "    #df['vector'] = output\n",
    "    vector_df = pd.DataFrame(output, index=df.index)\n",
    "    vector_df_norm = vector_df.div(vector_df.sum(axis=1), axis=0)\n",
    "    vector_df_norm['Label'] = df['Label']\n",
    "    vector_df_norm['TrainSubjectId'] = df['TrainSubjectId']\n",
    "    vector_df_norm = vector_df_norm.fillna(0)\n",
    "    corr = vector_df_norm.corr()\n",
    "    corr_label = corr['Label'].sort_values(ascending=False)\n",
    "    relevant_features = corr_label[1:16]\n",
    "    relevant_features_name = relevant_features.index.values\n",
    "    X = vector_df_norm[relevant_features_name]\n",
    "    y = vector_df_norm['Label']\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame(liwc_alike_output, index=data_input.index)\n",
    "b = a.div(a.sum(axis=1), axis=0)\n",
    "b = b.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "auxverb                 0.050534\n",
       "Cognitive Processes     0.023810\n",
       "Insight                 0.032718\n",
       "Inclusive               0.050858\n",
       "Work                    0.050373\n",
       "Perceptual Processes    0.018626\n",
       "Assent                  0.009718\n",
       "Pronouns                0.092485\n",
       "ipron                   0.047133\n",
       "Causation               0.028993\n",
       "Past tense              0.011014\n",
       "Seeing                  0.089893\n",
       "Feeling                 0.007937\n",
       "Articles                0.043408\n",
       "Certainty               0.013929\n",
       "Ppronouns               0.047943\n",
       "I                       0.015225\n",
       "Prepositions            0.045999\n",
       "Tentative               0.024943\n",
       "Exclusive               0.011824\n",
       "Affective Processes     0.001782\n",
       "Negative Emotions       0.005507\n",
       "Achievement             0.006479\n",
       "Social Processes        0.028993\n",
       "Friends                 0.002915\n",
       "Time                    0.001782\n",
       "Relativity              0.006641\n",
       "Motion                  0.012958\n",
       "Present tense           0.051344\n",
       "Discrepancy             0.010042\n",
       "Negations               0.011176\n",
       "Conjunctions            0.038711\n",
       "Shehe                   0.012310\n",
       "Humans                  0.007451\n",
       "They                    0.005831\n",
       "Adverbs                 0.012472\n",
       "Space                   0.005993\n",
       "We                      0.001620\n",
       "Positive Emotions       0.013120\n",
       "Health                  0.006641\n",
       "Anxiety                 0.000810\n",
       "Anger                   0.000648\n",
       "Hearing                 0.002592\n",
       "Numbers                 0.005507\n",
       "Future tense            0.005021\n",
       "Family                  0.002592\n",
       "Leisure                 0.003077\n",
       "You                     0.011500\n",
       "Sexuality               0.001134\n",
       "Body                    0.002106\n",
       "Inhibition              0.002430\n",
       "Home                    0.000486\n",
       "Biological Processes    0.000162\n",
       "Sadness                 0.000324\n",
       "Nonfluencies            0.000486\n",
       "Death                   0.000000\n",
       "Money                   0.000000\n",
       "Religion                0.000000\n",
       "Fillers                 0.000000\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.iloc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home_remote/pca.pkl']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save pca model\n",
    "pca = PCA(n_components=15)\n",
    "pca.fit(b)\n",
    "joblib.dump(pca, os.path.join(HOME_DIR,'pca.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home_remote/liwc_alike_pca.pkl']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(test_model, os.path.join(HOME_DIR,'liwc_alike_pca.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = pca.transform(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = test_model.predict(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.86      0.91       403\n",
      "           1       0.55      0.84      0.67        83\n",
      "\n",
      "    accuracy                           0.86       486\n",
      "   macro avg       0.76      0.85      0.79       486\n",
      "weighted avg       0.89      0.86      0.87       486\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#precision, recall, fscore\n",
    "print(classification_report(data_input['Label'], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8597251824180162"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model.predict_proba(c[4].reshape(1, -1))[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 165172\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Example text data (replace this with your actual text data)\n",
    "texts = data_input['text'].tolist()\n",
    "# Create a tokenizer and fit it on your text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Get the vocabulary size\n",
    "vocabulary_size = len(tokenizer.word_index) + 1  # Add 1 for the special padding token if used\n",
    "\n",
    "print(\"Vocabulary Size:\", vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(c, data_input['Label'], test_size=0.2, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 23:21:53.172728: W tensorflow/core/common_runtime/bfc_allocator.cc:462] Allocator (GPU_0_bfc) ran out of memory trying to allocate 243.0KiB (rounded to 248832)requested by op RandomUniform\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-11-13 23:21:53.172827: I tensorflow/core/common_runtime/bfc_allocator.cc:1010] BFCAllocator dump for GPU_0_bfc\n",
      "2023-11-13 23:21:53.172868: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (256): \tTotal Chunks: 4, Chunks in use: 3. 1.0KiB allocated for chunks. 768B in use in bin. 16B client-requested in use in bin.\n",
      "2023-11-13 23:21:53.172894: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-13 23:21:53.172916: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
      "2023-11-13 23:21:53.172936: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-13 23:21:53.172977: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-13 23:21:53.173000: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-13 23:21:53.173019: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-13 23:21:53.173045: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-13 23:21:53.173065: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-13 23:21:53.173084: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-13 23:21:53.173104: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-13 23:21:53.173123: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-13 23:21:53.173142: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-13 23:21:53.173161: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-13 23:21:53.173180: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-13 23:21:53.173201: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-13 23:21:53.173233: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-13 23:21:53.173266: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-13 23:21:53.173315: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (67108864): \tTotal Chunks: 2, Chunks in use: 2. 165.62MiB allocated for chunks. 165.62MiB in use in bin. 161.30MiB client-requested in use in bin.\n",
      "2023-11-13 23:21:53.173342: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-13 23:21:53.173361: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-13 23:21:53.173388: I tensorflow/core/common_runtime/bfc_allocator.cc:1033] Bin for 243.0KiB was 128.0KiB, Chunk State: \n",
      "2023-11-13 23:21:53.173405: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 173670400\n",
      "2023-11-13 23:21:53.173430: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 4203be0000 of size 256 next 1\n",
      "2023-11-13 23:21:53.173450: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 4203be0100 of size 1280 next 2\n",
      "2023-11-13 23:21:53.173466: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 4203be0600 of size 256 next 3\n",
      "2023-11-13 23:21:53.173482: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 4203be0700 of size 256 next 4\n",
      "2023-11-13 23:21:53.173498: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 4203be0800 of size 84568064 next 5\n",
      "2023-11-13 23:21:53.173515: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 4208c87000 of size 256 next 6\n",
      "2023-11-13 23:21:53.173534: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 4208c87100 of size 89100032 next 18446744073709551615\n",
      "2023-11-13 23:21:53.173550: I tensorflow/core/common_runtime/bfc_allocator.cc:1071]      Summary of in-use Chunks by size: \n",
      "2023-11-13 23:21:53.173568: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 3 Chunks of size 256 totalling 768B\n",
      "2023-11-13 23:21:53.173588: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2023-11-13 23:21:53.173621: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 84568064 totalling 80.65MiB\n",
      "2023-11-13 23:21:53.173654: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 89100032 totalling 84.97MiB\n",
      "2023-11-13 23:21:53.173696: I tensorflow/core/common_runtime/bfc_allocator.cc:1078] Sum Total of in-use chunks: 165.62MiB\n",
      "2023-11-13 23:21:53.173734: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] total_region_allocated_bytes_: 173670400 memory_limit_: 173670400 available bytes: 0 curr_region_allocation_bytes_: 347340800\n",
      "2023-11-13 23:21:53.173781: I tensorflow/core/common_runtime/bfc_allocator.cc:1086] Stats: \n",
      "Limit:                       173670400\n",
      "InUse:                       173670144\n",
      "MaxInUse:                    173670400\n",
      "NumAllocs:                           7\n",
      "MaxAllocSize:                 89100032\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2023-11-13 23:21:53.173826: W tensorflow/core/common_runtime/bfc_allocator.cc:474] **************************************************************************************************xx\n",
      "2023-11-13 23:21:53.173923: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at random_op.cc:74 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[486,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[486,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:RandomUniform]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m/home/thi.tra.my.nguyen/master_thesis/model_evaluation/lexicon_based_models.ipynb Cell 24\u001b[0m line \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsupmic2/home/thi.tra.my.nguyen/master_thesis/model_evaluation/lexicon_based_models.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m model2 \u001b[39m=\u001b[39m Sequential()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsupmic2/home/thi.tra.my.nguyen/master_thesis/model_evaluation/lexicon_based_models.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Embedding layer\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsupmic2/home/thi.tra.my.nguyen/master_thesis/model_evaluation/lexicon_based_models.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m model2\u001b[39m.\u001b[39;49madd(Embedding(input_dim \u001b[39m=\u001b[39;49m \u001b[39m486\u001b[39;49m , output_dim \u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsupmic2/home/thi.tra.my.nguyen/master_thesis/model_evaluation/lexicon_based_models.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# LSTM layer with ReLU activation and dropout\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsupmic2/home/thi.tra.my.nguyen/master_thesis/model_evaluation/lexicon_based_models.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m model2\u001b[39m.\u001b[39madd(LSTM(\u001b[39m128\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m, dropout\u001b[39m=\u001b[39m\u001b[39m0.3\u001b[39m, recurrent_dropout\u001b[39m=\u001b[39m\u001b[39m0.3\u001b[39m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/tracking/base.py:629\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 629\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    630\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/backend.py:1920\u001b[0m, in \u001b[0;36mRandomGenerator.random_uniform\u001b[0;34m(self, shape, minval, maxval, dtype)\u001b[0m\n\u001b[1;32m   1917\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generator:\n\u001b[1;32m   1918\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generator\u001b[39m.\u001b[39muniform(\n\u001b[1;32m   1919\u001b[0m       shape\u001b[39m=\u001b[39mshape, minval\u001b[39m=\u001b[39mminval, maxval\u001b[39m=\u001b[39mmaxval, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m-> 1920\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49muniform(\n\u001b[1;32m   1921\u001b[0m     shape\u001b[39m=\u001b[39;49mshape, minval\u001b[39m=\u001b[39;49mminval, maxval\u001b[39m=\u001b[39;49mmaxval, dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   1922\u001b[0m     seed\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmake_legacy_seed())\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[486,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:RandomUniform]"
     ]
    }
   ],
   "source": [
    "#lstm \n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# Define the model\n",
    "model2 = Sequential()\n",
    "\n",
    "# Embedding layer\n",
    "model2.add(Embedding(input_dim = 15 , output_dim =128, sequential_length=15))\n",
    "\n",
    "# LSTM layer with ReLU activation and dropout\n",
    "model2.add(LSTM(128, activation='relu', dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "# Fully connected layer with ReLU activation\n",
    "model2.add(Dense(128, activation='relu', kernel_regularizer='l2', bias_regularizer='l2'))\n",
    "\n",
    "# Dropout layer\n",
    "model2.add(Dropout(0.3))\n",
    "\n",
    "# Output layer with softmax activation for binary classification\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model2.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=1e-4), metrics=['accuracy'])\n",
    "\n",
    "# Learning rate decay scheduler\n",
    "def lr_scheduler(epoch, lr):\n",
    "    return lr * np.exp(-1e-5 * epoch)\n",
    "\n",
    "lr_callback = LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# Train the model\n",
    "model2.fit(X_train, y_train, epochs=130, batch_size=100, callbacks=[lr_callback], class_weight={0: 0.2, 1: 0.8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
