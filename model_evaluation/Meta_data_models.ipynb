{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/thi.tra.my.nguyen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/thi.tra.my.nguyen/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "HOME_DIR = \"/home_remote\"\n",
    "HOME = \"/home/thi.tra.my.nguyen\"\n",
    "\n",
    "from liwc import Liwc\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize, ngrams\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#import logisitic_regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "import textstat\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives_df_path = os.path.join(HOME_DIR, \"positive_df.pkl\")\n",
    "negatives_df_path = os.path.join(HOME_DIR, \"negative_df.pkl\")\n",
    "\n",
    "positives = pd.read_pickle(positives_df_path)\n",
    "negatives = pd.read_pickle(negatives_df_path)\n",
    "positives['Date'] = pd.to_datetime(positives['Date'])\n",
    "negatives['Date'] = pd.to_datetime(negatives['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(text, n):\n",
    "  n_grams = ngrams(word_tokenize(text), n)\n",
    "  return [ ' '.join(grams) for grams in n_grams]\n",
    "\n",
    "def frequency_distribution(grams, word):\n",
    "    ls = []\n",
    "    for i in grams:\n",
    "        count = 0\n",
    "        for j in i:\n",
    "            if j == word:\n",
    "                count += 1\n",
    "        ls.append(count)\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count number of words in pos_tagging\n",
    "def count_word_pos_tagging(pos_tagging, word):\n",
    "    count = 0\n",
    "    for i in pos_tagging:\n",
    "        if i[1] == word:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count number of words in pos_tagging but word in the list\n",
    "def count_word_list(tokens, list_word):\n",
    "    count = 0\n",
    "    for i in tokens:\n",
    "        if i in list_word:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_liwc_input_crafted(df, label):\n",
    "\n",
    "  df['text'] = df['Text']+ df['Title']\n",
    "  df['Token'] = df['text'].apply(lambda x: word_tokenize(x))\n",
    "  df['AVG_SEN'] = df['text'].apply(lambda x:  textstat.avg_sentence_length(x))\n",
    "  df['AVG_PER_WORD'] = df['text'].apply(lambda x:  textstat.avg_letter_per_word(x))\n",
    "  df['LWF'] = df['text'].apply(lambda x: textstat.linsear_write_formula(x))\n",
    "  df['FRE'] = df['text'].apply(lambda x: textstat.flesch_reading_ease(x))\n",
    "  df['DCR'] = df['text'].apply(lambda x: textstat.dale_chall_readability_score(x))\n",
    "  df['FOG'] = df['text'].apply(lambda x: textstat.gunning_fog(x))\n",
    "  \n",
    "\n",
    "  #count number of I, my depression, my anxiety, in the text\n",
    "  bigrams = df['Text'].apply(lambda x: get_ngrams(x, 2))\n",
    "  unigrams = df['Text'].apply(lambda x: get_ngrams(x, 1))\n",
    "  unigrams_title = df['Title'].apply(lambda x: get_ngrams(x, 1))\n",
    "  depression = frequency_distribution(bigrams, 'my depression')\n",
    "  anxiety = frequency_distribution(bigrams, 'my anxiety')\n",
    "  therapist = frequency_distribution(bigrams, 'my therapist')\n",
    "  count_I = frequency_distribution(unigrams, 'I')\n",
    "  count_I_title = frequency_distribution(unigrams_title, 'I')\n",
    "  df['My_Therapist'] = therapist\n",
    "  df['My_Depression'] = depression\n",
    "  df['My_Anxiety'] = anxiety\n",
    "  df['word_I'] = count_I\n",
    "  df['word_I_title'] = count_I_title\n",
    "  #count if unigrams contain \"Zoloft\", \"Celexa\", \"Lexapro\", \"Paxil\", \"Pexeva\", \"Brisdelle\", \"Luvox\"\n",
    "  antidepression = [\"Zoloft\", \"Celexa\", \"Lexapro\", \"Paxil\", \"Pexeva\", \"Brisdelle\", \"Luvox\"]\n",
    "  df['Antidepressants'] = df['Token'].apply(lambda x: count_word_list(x, antidepression))\n",
    "\n",
    "\n",
    "\n",
    "  #return boolean if the text contains \"I was diagnosed with depression\" or \"I was diagnosed with anxiety\" or \"I've been diagnosed with depression\"\n",
    "  df['Diagnosed_Depression'] = df['Text'].apply(lambda x: 1 if 'I was diagnosed with depression' in x or 'I was diagnosed with anxiety' in x or 'I\\'ve been diagnosed with depression' in x else 0)\n",
    "\n",
    "  #POS tagging to count number of possessive pronouns, personal pronouns, past tense verbs.\n",
    "  temp = df['Token'].apply(lambda x: nltk.pos_tag(x))\n",
    "  df['POS'] = [count_word_pos_tagging(i, 'PRP$') for i in temp]\n",
    "  df['PRP'] = [count_word_pos_tagging(i, 'PRP') for i in temp]\n",
    "  df['VBD'] = [count_word_pos_tagging(i, 'VBD') for i in temp]\n",
    "\n",
    "  # calculate avergae length of word of title per user\n",
    "  df['Length_Title'] = df['Title'].apply(lambda x: len(word_tokenize(x)))\n",
    "\n",
    "  #get month of each writing\n",
    "  df['Month'] = df['Date'].apply(lambda x: x.month)\n",
    "  #get hour of each writing\n",
    "  df['Hour'] = df['Date'].apply(lambda x: x.hour)\n",
    "\n",
    "\n",
    "  result_df = df.groupby('TrainSubjectId').agg({'POS':'mean', 'PRP':'mean', 'VBD':'mean','Length_Title': 'mean', 'Month':'mean','Hour':'mean','LWF': 'mean', 'FRE': 'mean', 'DCR': 'mean', 'FOG': 'mean','AVG_SEN':'mean', 'AVG_PER_WORD': 'mean','My_Depression':'sum','My_Anxiety':'sum','My_Therapist':'sum','word_I':'mean','word_I_title':'mean','Diagnosed_Depression':'sum' ,'Antidepressants':'sum','Text':'count'}).reset_index()\n",
    "  result_df[\"Label\"] = label\n",
    " \n",
    "  #join text per user\n",
    "  joined_text_df = df.groupby('TrainSubjectId')['text'].apply(' '.join).reset_index()\n",
    "  result_df = result_df.merge(joined_text_df, on=\"TrainSubjectId\")\n",
    "\n",
    "  # number_of_writings_df = df.groupby('TrainSubjectId')['Text'].apply(lambda x: len(x)).reset_index()\n",
    "  result_df.rename(columns={'Text': 'NumOfWritings'}, inplace=True)\n",
    "\n",
    "  # #merge number of writings and result_df on trainSubjectId\n",
    "  # result_df_final = result_df.merge(number_of_writings_df, on=\"TrainSubjectId\")\n",
    "  \n",
    "  return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives_liwc = construct_liwc_input_crafted(positives, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives_liwc = construct_liwc_input_crafted(negatives, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate two dataframes\n",
    "liwc_df = pd.concat([positives_liwc, negatives_liwc], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_df.to_pickle(os.path.join(HOME_DIR, \"liwc_df_full_crafted.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TrainSubjectId', 'POS', 'PRP', 'VBD', 'Length_Title', 'Month', 'Hour',\n",
       "       'LWF', 'FRE', 'DCR', 'FOG', 'AVG_SEN', 'AVG_PER_WORD', 'My_Depression',\n",
       "       'My_Anxiety', 'My_Therapist', 'word_I', 'word_I_title',\n",
       "       'Diagnosed_Depression', 'Antidepressants', 'NumOfWritings', 'Label',\n",
       "       'text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liwc_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract features and handcrafted features\n",
    "def get_features_crafted(df,output, type):\n",
    "    hand_crafted = [\n",
    "        'POS', 'PRP', 'VBD', 'Length_Title', 'Month', 'Hour',\n",
    "       'LWF', 'FRE', 'DCR', 'FOG', 'AVG_SEN', 'AVG_PER_WORD', 'My_Depression',\n",
    "       'My_Anxiety', 'My_Therapist', 'word_I', 'word_I_title',\n",
    "       'Diagnosed_Depression', 'Antidepressants', 'NumOfWritings']\n",
    "    \n",
    "    relevant_features_name ={'liwc': ['i', 'friend', 'sad', 'family', 'feel', 'health',\n",
    "       'sexual', 'anx', 'body', 'bio', 'ppron', 'filler', 'shehe', 'adverb',\n",
    "       'swear', 'humans', 'excl', 'assent', 'discrep', 'you', 'pronoun',\n",
    "       'negemo', 'past'],\n",
    "                        'liwc_alike': ['Anxiety', 'I', 'Sadness', 'Affective Processes',\n",
    "       'Sexuality', 'Family', 'Friends', 'Fillers', 'Health', 'Feeling',\n",
    "       'Humans', 'Biological Processes', 'Time', 'Body', 'Negative Emotions',\n",
    "       'Social Processes', 'Perceptual Processes', 'Insight',\n",
    "       'Cognitive Processes', 'Motion', 'Positive Emotions', 'Tentative',\n",
    "       'Ppronouns']}\n",
    "    vector_df = pd.DataFrame(output, index=df.index)\n",
    "    #vector_df_norm = vector_df.div(vector_df.sum(axis=1), axis=0)\n",
    "    #vector_df_norm['Label'] = df['Label']\n",
    "    #vector_df_norm['TrainSubjectId'] = df['TrainSubjectId']\n",
    "    vector_df= vector_df.fillna(0)\n",
    "    #corr = vector_df_norm.corr()\n",
    "    #corr_label = corr['Label'].sort_values(ascending=False)\n",
    "    #relevant_features = corr_label[1:15]\n",
    "    #relevant_features_name = relevant_features.index.values\n",
    "    re = vector_df[relevant_features_name[type]]\n",
    "    for i in hand_crafted:\n",
    "        re[i] = df[i]\n",
    "    X = re\n",
    "    y = df['Label']\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run through LIWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LIWC dictionary\n",
    "liwc = Liwc(os.path.join(HOME_DIR, \"master_thesis/LIWC2007_English100131.dic\"))\n",
    "input = [liwc.parse(word_tokenize(text)) for text in liwc_df['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LIWC-alike dictionary\n",
    "%run /home_remote/master_thesis/model_evaluation/liwc_alike.py\n",
    "liwc_alike_output = [main(text, result) for text in liwc_df['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_alike, y_alike = get_features_crafted(liwc_df, liwc_alike_output, 'liwc_alike')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_alike_norm = scaler.fit_transform(X_alike)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home_remote/scaler_alike.pkl']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(scaler, os.path.join(HOME_DIR, \"scaler_alike.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.87      0.91       403\n",
      "           1       0.57      0.84      0.68        83\n",
      "\n",
      "    accuracy                           0.86       486\n",
      "   macro avg       0.77      0.86      0.80       486\n",
      "weighted avg       0.90      0.86      0.87       486\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(C=64, penalty='l1', solver='liblinear', class_weight={0: 0.2, 1: 0.8})\n",
    "model.fit(X_alike_norm, y_alike)\n",
    "y_alike_pred = model.predict(X_alike_norm)\n",
    "print(classification_report(y_alike, y_alike_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6796116504854369\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(y_alike, y_alike_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home_remote/liwc_alike_full_crafted.pkl']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model, os.path.join(HOME_DIR, \"liwc_alike_full_crafted.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for LIWC\n",
    "X_liwc, y_liwc = get_features_crafted(liwc_df, input, 'liwc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler2 = StandardScaler()\n",
    "X_liwc_norm = scaler2.fit_transform(X_liwc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home_remote/scaler_liwc.pkl']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(scaler2, os.path.join(HOME_DIR, \"scaler_liwc.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.86      0.91       403\n",
      "           1       0.56      0.86      0.68        83\n",
      "\n",
      "    accuracy                           0.86       486\n",
      "   macro avg       0.76      0.86      0.79       486\n",
      "weighted avg       0.90      0.86      0.87       486\n",
      "\n",
      "0.6761904761904762\n"
     ]
    }
   ],
   "source": [
    "mod= LogisticRegression(C=64, penalty='l1', solver='liblinear', class_weight={0: 0.2, 1: 0.8})\n",
    "mod.fit(X_liwc_norm, y_liwc)\n",
    "y_liwc_pred = mod.predict(X_liwc_norm)\n",
    "print(classification_report(y_liwc, y_liwc_pred))\n",
    "print(f1_score(y_liwc, y_liwc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home_remote/liwc_full_crafted.pkl']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(mod, os.path.join(HOME_DIR, \"liwc_full_crafted.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get 10 features of LIWC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_crafted_10(df,output, type):\n",
    "    hand_crafted = [\n",
    "        'POS', 'PRP', 'VBD', 'Length_Title', 'Month', 'Hour',\n",
    "       'LWF', 'FRE', 'DCR', 'FOG', 'AVG_SEN', 'AVG_PER_WORD', 'My_Depression',\n",
    "       'My_Anxiety', 'My_Therapist', 'word_I', 'word_I_title',\n",
    "       'Diagnosed_Depression', 'Antidepressants', 'NumOfWritings']\n",
    "    \n",
    "    relevant_features_name ={'liwc': ['i', 'friend', 'sad','sexual', 'anx','ppron', 'discrep', 'pronoun','negemo', 'past'],\n",
    "                        'liwc_alike': ['Anxiety', 'I', 'Sadness', 'Negative Emotions','Social Processes', 'Insight','Cognitive Processes', 'Motion', 'Positive Emotions','Ppronouns']}\n",
    "    vector_df = pd.DataFrame(output, index=df.index)\n",
    "    #vector_df_norm = vector_df.div(vector_df.sum(axis=1), axis=0)\n",
    "    #vector_df_norm['Label'] = df['Label']\n",
    "    #vector_df_norm['TrainSubjectId'] = df['TrainSubjectId']\n",
    "    vector_df= vector_df.fillna(0)\n",
    "    #corr = vector_df_norm.corr()\n",
    "    #corr_label = corr['Label'].sort_values(ascending=False)\n",
    "    #relevant_features = corr_label[1:15]\n",
    "    #relevant_features_name = relevant_features.index.values\n",
    "    re = vector_df[relevant_features_name[type]]\n",
    "    for i in hand_crafted:\n",
    "        re[i] = df[i]\n",
    "    X = re\n",
    "    y = df['Label']\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_alike_10, y_alike_10 = get_features_crafted_10(liwc_df, liwc_alike_output, 'liwc_alike')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_alike_10= StandardScaler()\n",
    "X_alike_norm_10 = scaler_alike_10.fit_transform(X_alike_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94       403\n",
      "           1       0.71      0.66      0.69        83\n",
      "\n",
      "    accuracy                           0.90       486\n",
      "   macro avg       0.82      0.80      0.81       486\n",
      "weighted avg       0.89      0.90      0.90       486\n",
      "\n",
      "0.6875000000000001\n"
     ]
    }
   ],
   "source": [
    "mod_alike_10 = LogisticRegression(C=64, penalty='l2', class_weight={0: 1/3, 1: 2/3})\n",
    "mod_alike_10.fit(X_alike_norm_10, y_alike_10)\n",
    "y_alike_pred_10 = mod_alike_10.predict(X_alike_norm_10)\n",
    "print(classification_report(y_alike_10, y_alike_pred_10))\n",
    "print(f1_score(y_alike_10, y_alike_pred_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home_remote/liwc_alike_10_full_crafted.pkl']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(mod_alike_10, os.path.join(HOME_DIR, \"liwc_alike_10_full_crafted.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home_remote/scaler_alike_10.pkl']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(scaler_alike_10, os.path.join(HOME_DIR, \"scaler_alike_10.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93       403\n",
      "           1       0.67      0.60      0.63        83\n",
      "\n",
      "    accuracy                           0.88       486\n",
      "   macro avg       0.79      0.77      0.78       486\n",
      "weighted avg       0.88      0.88      0.88       486\n",
      "\n",
      "0.6329113924050633\n"
     ]
    }
   ],
   "source": [
    "X_liwc_10, y_liwc_10 = get_features_crafted_10(liwc_df, input, 'liwc')\n",
    "scaler_liwc_10 = StandardScaler()\n",
    "X_liwc_norm_10 = scaler_liwc_10.fit_transform(X_liwc_10)\n",
    "mod_liwc_10 = LogisticRegression(C=64, penalty='l2', class_weight={0: 1/3, 1: 2/3})\n",
    "mod_liwc_10.fit(X_liwc_norm_10, y_liwc_10)\n",
    "y_liwc_pred_10 = mod_liwc_10.predict(X_liwc_norm_10)\n",
    "print(classification_report(y_liwc_10, y_liwc_pred_10))\n",
    "print(f1_score(y_liwc_10, y_liwc_pred_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home_remote/liwc_10_full_crafted.pkl']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(scaler_liwc_10, os.path.join(HOME_DIR, \"scaler_liwc_10.pkl\"))\n",
    "joblib.dump(mod_liwc_10, os.path.join(HOME_DIR, \"liwc_10_full_crafted.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
