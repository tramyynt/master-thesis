{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install tensorflow==2.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-09 00:44:54.078112: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-09 00:44:56.048358: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-12-09 00:44:56.048498: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-12-09 00:44:56.048516: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "HOME_DIR = \"/home_remote\"\n",
    "HOME = \"/home/thi.tra.my.nguyen\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv1D, Dense, MaxPooling1D, Flatten, Dropout, Activation, Input, Embedding, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import Constant\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "1 Physical GPUs; 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-09 00:44:59.119495: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-09 00:44:59.750521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10785 MB memory:  -> device: 0, name: Tesla K80, pci bus id: 0000:8d:00.0, compute capability: 3.7\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    # tf.config.set_logical_device_configuration(\n",
    "    #     gpus[0],\n",
    "    #     [tf.config.LogicalDeviceConfiguration(memory_limit=9216)])\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs;\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Wiki pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "ft = fasttext.load_model('/home_remote/fastText/cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get word embeddings for a document\n",
    "def get_document_embeddings(document, max_length=100):\n",
    "    # Tokenize the document and get the first max_length word vectors\n",
    "    tokens = document.split()[:max_length]\n",
    "    \n",
    "    # Apply zero-padding if the document is shorter than max_length\n",
    "    if len(tokens) < max_length:\n",
    "        padding_count = max_length - len(tokens)\n",
    "        tokens.extend(['<PAD>'] * padding_count)\n",
    "\n",
    "    # Get word embeddings for each token\n",
    "    embeddings = [ft.get_word_vector(token) for token in tokens]\n",
    "\n",
    "    return np.array(embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess train & test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives_df_path = os.path.join(HOME_DIR, \"positive_df.pkl\")\n",
    "negatives_df_path = os.path.join(HOME_DIR, \"negative_df.pkl\")\n",
    "\n",
    "positives = pd.read_pickle(positives_df_path)\n",
    "negatives = pd.read_pickle(negatives_df_path)\n",
    "\n",
    "positives['Label'] = 1\n",
    "negatives['Label'] = 0\n",
    "\n",
    "#concatenate the two dataframes\n",
    "df = pd.concat([positives, negatives], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = df.TrainSubjectId.unique()\n",
    "#split the subjects into train and test\n",
    "train_subject, test_subject = train_test_split(subject, test_size=0.2, random_state=42, shuffle=True)\n",
    "df_train = df[df.TrainSubjectId.isin(train_subject)]\n",
    "df_test = df[df.TrainSubjectId.isin(test_subject)]\n",
    "df_train = df_train.sample(frac=1, random_state=42)\n",
    "df_test = df_test.sample(frac=1, random_state=42)\n",
    "y_train = df_train.Label\n",
    "y_test = df_test.Label\n",
    "X_train = df_train['Text']\n",
    "X_test = df_test['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((237427, 100, 300), (59608, 100, 300))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#convert X_train to word embeddings using get_document_embeddings function\n",
    "X_train = np.array([get_document_embeddings(document) for document in X_train])\n",
    "X_test = np.array([get_document_embeddings(document) for document in X_test])\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((237427, 1), (59608, 1))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert y_train and y_test to numpy array with shape (n, 1)\n",
    "y_train = np.array(y_train).reshape(-1, 1)\n",
    "y_test = np.array(y_test).reshape(-1, 1)\n",
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "## Save train and test data to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_to_disk(np_array, file_name_prefix, file_type=\"npy\"):\n",
    "    \"\"\"\n",
    "    Split the np_array into 10 chunks and save them to csv files\n",
    "    \"\"\"\n",
    "    chunks = np.array_split(np_array, 10)\n",
    "    for i in range(len(chunks)):\n",
    "        filename = os.path.join(HOME_DIR, \"master_thesis/data\", \"{}_{}.{}\".format(file_name_prefix, i + 1, file_type))\n",
    "        if file_type == \"npy\":\n",
    "            np.save(filename, chunks[i])\n",
    "        elif file_type == \"csv\":\n",
    "            np.savetxt(filename, chunks[i])\n",
    "        else:\n",
    "            raise ValueError(\"file_type must be either npy or csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_2D = X_train.reshape(X_train.shape[0], -1)\n",
    "# save_data_to_disk(X_train_2D, \"X_train_2D\")\n",
    "# X_test_2D = X_test.reshape(X_test.shape[0], -1)\n",
    "# save_data_to_disk(X_test_2D, \"X_test_2D\")\n",
    "\n",
    "# save_data_to_disk(y_train, \"y_train\")\n",
    "# save_data_to_disk(y_test, \"y_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data_to_disk(X_train, \"X_train\", file_type=\"npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data_to_disk(y_train, \"y_train\", file_type=\"npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `tf.data.Dataset` object to store train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_npy_file(X_path, y_path):\n",
    "#     X = np.load(X_path)\n",
    "#     y = np.load(y_path)\n",
    "#     return X, y, X.shape, y.shape\n",
    "\n",
    "# def load_and_parse_data_from_npy_file(X_path, y_path):\n",
    "#     # \"temp[0]\" is X, \"temp[1]\" is y, \"temp[2]\" is X.shape, \"temp[3]\" is y.shape\n",
    "#     temp = tf.py_function(read_npy_file, inp=[X_path, y_path], Tout=(tf.float32, tf.int32, tf.int32, tf.int32))\n",
    "#     X_tensor = tf.reshape(temp[0], temp[2])\n",
    "#     y_tensor = tf.reshape(temp[1], temp[3])\n",
    "#     return X_tensor, y_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_file_pattern = os.path.join(HOME_DIR, \"master_thesis/data/X_*.npy\")\n",
    "# X_file_list = glob.glob(X_file_pattern)\n",
    "# X_file_list.sort()\n",
    "\n",
    "# y_file_pattern = os.path.join(HOME_DIR, \"master_thesis/data/y_*.npy\")\n",
    "# y_file_list = glob.glob(y_file_pattern)\n",
    "# y_file_list.sort()\n",
    "\n",
    "# train_dataset_file_paths = tf.data.Dataset.from_tensor_slices((X_file_list, y_file_list))\n",
    "# train_dataset = train_dataset_file_paths.map(load_and_parse_data_from_npy_file)\n",
    "# train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_function(X_train_path, y_train_path):\n",
    "    X_train_file_list = glob.glob(X_train_path)\n",
    "    X_train_file_list.sort()\n",
    "    y_train_file_list = glob.glob(y_train_path)\n",
    "    y_train_file_list.sort()\n",
    "\n",
    "    for npy_file_x, npy_file_y in zip(X_train_file_list, y_train_file_list):\n",
    "        data_x = np.load(npy_file_x, mmap_mode='r')\n",
    "        data_y = np.load(npy_file_y, mmap_mode='r')\n",
    "\n",
    "        batch_size = 32\n",
    "        for i in range(0, len(data_x), batch_size):\n",
    "            yield data_x[i:i + batch_size], data_y[i:i + batch_size]\n",
    "\n",
    "X_train_filename_pattern = os.path.join(HOME_DIR, \"master_thesis/data/X_train*.npy\")\n",
    "X_train_shape = (None, 100, 300)\n",
    "y_train_filename_pattern = os.path.join(HOME_DIR, \"master_thesis/data/y_train*.npy\")\n",
    "y_train_shape = (None, 1)\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    generator_function, \n",
    "    args=(X_train_filename_pattern, y_train_filename_pattern), \n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=X_train_shape, dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=y_train_shape, dtype=tf.int32)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Sequential models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom CReLU activation function\n",
    "def crelu(x):\n",
    "    pos = K.relu(x)\n",
    "    neg = K.relu(-x)\n",
    "    return K.concatenate([pos, neg], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_3 (Conv1D)           (None, 99, 100)           60100     \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 99, 200)           0         \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 99, 200)           0         \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPooling  (None, 1, 200)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 200)               40200     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 200)               20100     \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 100)               10050     \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 50)                2525      \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 2)                 102       \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 133,080\n",
      "Trainable params: 133,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Word embedding dimensionality\n",
    "word_vector_dimensions = 300\n",
    "\n",
    "# Model\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional Layer\n",
    "model.add(Conv1D(filters=100, kernel_size=2, input_shape=(100, 300)))\n",
    "model.add(Activation(crelu))\n",
    "model.add(Activation(lambda x: x * -1))  # Negated activation for CReLU\n",
    "\n",
    "# 1-Max Pooling Layer\n",
    "model.add(MaxPooling1D(pool_size=99))\n",
    "\n",
    "# Fully Connected Layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(200))\n",
    "\n",
    "# Dropout Layer\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# 3 Fully Connected Layers\n",
    "model.add(Dense(100, activation=crelu))\n",
    "model.add(Dense(50, activation=crelu))\n",
    "model.add(Dense(25, activation=crelu))\n",
    "\n",
    "# Final Layer\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=1e-4), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model with `tf.data.DataSet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(dataset, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7f145864a8b0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_dataset = test_dataset.batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.00728162, -0.05424538,  0.03505474, ...,  0.00139213,\n",
       "         -0.1551645 , -0.08451691],\n",
       "        [ 0.00735422, -0.0123115 ,  0.00735966, ...,  0.07680896,\n",
       "         -0.00778688,  0.03561359],\n",
       "        [ 0.00823911, -0.08990277,  0.02652529, ..., -0.01159137,\n",
       "         -0.04112864,  0.03625222],\n",
       "        ...,\n",
       "        [-0.00627562, -0.02137895,  0.01162977, ..., -0.02386858,\n",
       "          0.02757748, -0.00151885],\n",
       "        [-0.00627562, -0.02137895,  0.01162977, ..., -0.02386858,\n",
       "          0.02757748, -0.00151885],\n",
       "        [-0.00627562, -0.02137895,  0.01162977, ..., -0.02386858,\n",
       "          0.02757748, -0.00151885]]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = \"i'm depressed and want to kill myself.\"\n",
    "vc = get_document_embeddings(txt)\n",
    "vc.reshape(1, 100, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(vc.reshape(1, 100, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
