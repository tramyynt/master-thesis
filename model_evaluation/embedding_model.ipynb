{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install tensorflow==2.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 14:06:38.602074: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-10 14:06:41.951125: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-12-10 14:06:41.951250: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-12-10 14:06:41.951267: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "HOME_DIR = \"/home_remote\"\n",
    "HOME = \"/home/thi.tra.my.nguyen\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv1D, Dense, MaxPooling1D, Flatten, Dropout, Activation, Input, Embedding, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import Constant\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "1 Physical GPUs; 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 14:06:47.488936: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-10 14:06:48.086487: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10785 MB memory:  -> device: 0, name: Tesla K80, pci bus id: 0000:8d:00.0, compute capability: 3.7\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    # tf.config.set_logical_device_configuration(\n",
    "    #     gpus[0],\n",
    "    #     [tf.config.LogicalDeviceConfiguration(memory_limit=9216)])\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs;\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess train & test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Wiki pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "ft = fasttext.load_model('/home_remote/fastText/cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to get word embeddings for a document\n",
    "# def get_document_embeddings(document, max_length=100):\n",
    "#     # Tokenize the document and get the first max_length word vectors\n",
    "#     tokens = document.split()[:max_length]\n",
    "    \n",
    "#     # Apply zero-padding if the document is shorter than max_length\n",
    "#     if len(tokens) < max_length:\n",
    "#         padding_count = max_length - len(tokens)\n",
    "#         tokens.extend(['<PAD>'] * padding_count)\n",
    "\n",
    "#     # Get word embeddings for each token\n",
    "#     embeddings = [ft.get_word_vector(token) for token in tokens]\n",
    "\n",
    "#     return np.array(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives_df_path = os.path.join(HOME_DIR, \"positive_df.pkl\")\n",
    "negatives_df_path = os.path.join(HOME_DIR, \"negative_df.pkl\")\n",
    "\n",
    "positives = pd.read_pickle(positives_df_path)\n",
    "negatives = pd.read_pickle(negatives_df_path)\n",
    "\n",
    "positives['Label'] = 1\n",
    "negatives['Label'] = 0\n",
    "\n",
    "#concatenate the two dataframes\n",
    "df = pd.concat([positives, negatives], ignore_index=True)\n",
    "df['text'] = df['Text'] + df['Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle the dataframe and train-test split\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "X = df['text']\n",
    "y = df['Label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert y_train and y_test to numpy arrays\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents_matrix(documents, max_words=100, embedding_dim=300):\n",
    "    document_matrices = []\n",
    "\n",
    "    for document in documents:\n",
    "        # Split document into words\n",
    "        words = document.split()[:max_words]\n",
    "\n",
    "        # Get word embeddings using FastText\n",
    "        embeddings = [ft.get_word_vector(word) for word in words]\n",
    "\n",
    "        document_matrices.append(embeddings)\n",
    "\n",
    "    # Pad each sequence of embeddings to a common length\n",
    "    padded_document_matrices = pad_sequences(document_matrices, maxlen=max_words, dtype='float32', padding='post', truncating='post')\n",
    "\n",
    "    return padded_document_matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_emdedded = get_documents_matrix(X_train)\n",
    "X_test_emdedded = get_documents_matrix(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the whole dataset to embedding\n",
    "X_embedded = get_documents_matrix(X)\n",
    "#convert to array and reshape y to 2D\n",
    "y_embedded= np.array(y).reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "## Save train and test data to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_to_disk(np_array, file_name_prefix, file_type=\"npy\"):\n",
    "    \"\"\"\n",
    "    Split the np_array into 10 chunks and save them to csv files\n",
    "    \"\"\"\n",
    "    chunks = np.array_split(np_array, 10)\n",
    "    for i in range(len(chunks)):\n",
    "        filename = os.path.join(HOME_DIR, \"master_thesis/data\", \"{}_{}.{}\".format(file_name_prefix, i + 1, file_type))\n",
    "        if file_type == \"npy\":\n",
    "            np.save(filename, chunks[i])\n",
    "        elif file_type == \"csv\":\n",
    "            np.savetxt(filename, chunks[i])\n",
    "        else:\n",
    "            raise ValueError(\"file_type must be either npy or csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data_to_disk(X_train_emdedded, \"X_train_emdedded\", file_type=\"npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data_to_disk(y_train, \"y_train\", file_type=\"npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data_to_disk(X_embedded, \"X_embedded\", file_type=\"npy\")\n",
    "save_data_to_disk(y_embedded, \"y_embedded\", file_type=\"npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `tf.data.Dataset` object to store train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_function(X_train_path, y_train_path):\n",
    "    X_train_file_list = glob.glob(X_train_path)\n",
    "    X_train_file_list.sort()\n",
    "    y_train_file_list = glob.glob(y_train_path)\n",
    "    y_train_file_list.sort()\n",
    "\n",
    "    for npy_file_x, npy_file_y in zip(X_train_file_list, y_train_file_list):\n",
    "        data_x = np.load(npy_file_x, mmap_mode='r')\n",
    "        data_y = np.load(npy_file_y, mmap_mode='r')\n",
    "\n",
    "        batch_size = 32\n",
    "        for i in range(0, len(data_x), batch_size):\n",
    "            yield data_x[i:i + batch_size], data_y[i:i + batch_size]\n",
    "\n",
    "X_train_filename_pattern = os.path.join(HOME_DIR, \"master_thesis/data/X_embedded*.npy\")\n",
    "X_train_shape = (None, 100, 300)\n",
    "y_train_filename_pattern = os.path.join(HOME_DIR, \"master_thesis/data/y_embedded*.npy\")\n",
    "y_train_shape = (None, 1)\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    generator_function, \n",
    "    args=(X_train_filename_pattern, y_train_filename_pattern), \n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=X_train_shape, dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=y_train_shape, dtype=tf.int32)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Sequential models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom CReLU activation function\n",
    "def crelu(x):\n",
    "    pos = K.relu(x)\n",
    "    neg = K.relu(-x)\n",
    "    return K.concatenate([pos, neg], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word embedding dimensionality\n",
    "word_vector_dimensions = 300\n",
    "\n",
    "# Model\n",
    "model2 = Sequential()\n",
    "\n",
    "# Convolutional Layer\n",
    "model2.add(Conv1D(filters=100, kernel_size=2, input_shape=(100, 300)))\n",
    "model2.add(Activation(crelu))\n",
    "model2.add(Activation(lambda x: x * -1))  # Negated activation for CReLU\n",
    "\n",
    "# 1-Max Pooling Layer\n",
    "model2.add(MaxPooling1D(pool_size=99))\n",
    "\n",
    "# Fully Connected Layer\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(200))\n",
    "\n",
    "# Dropout Layer\n",
    "model2.add(Dropout(0.4))\n",
    "\n",
    "# 3 Fully Connected Layers\n",
    "model2.add(Dense(100, activation=crelu))\n",
    "model2.add(Dense(50, activation=crelu))\n",
    "model2.add(Dense(25, activation=crelu))\n",
    "\n",
    "# Final Layer\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "# Display the model summary\n",
    "model2.summary()\n",
    "\n",
    "model2.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=1e-4), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model with `tf.data.DataSet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.fit(dataset, epochs=30, batch_size=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "model2.save(os.path.join(HOME_DIR, \"master_thesis/cnn_model2.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = [X_test[48987]]\n",
    "vc = get_documents_matrix(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.models.save_model(my_model, './saved_models/my_tf_model')\n",
    "model2.save(os.path.join(HOME_DIR, \"master_thesis/cnn_model2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_5 (Conv1D)           (None, 99, 100)           60100     \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 99, 200)           0         \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 99, 200)           0         \n",
      "                                                                 \n",
      " max_pooling1d_5 (MaxPooling  (None, 1, 200)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 200)               40200     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 200)               20100     \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 100)               10050     \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 50)                2525      \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 133,026\n",
      "Trainable params: 133,026\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_tf_saved_model = tf.keras.models.load_model(os.path.join(HOME_DIR, \"master_thesis/cnn_model2\"))\n",
    "my_tf_saved_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 14:11:18.426907: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8200\n",
      "2023-12-10 14:11:18.731953: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.92GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1857/1857 [==============================] - 13s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 14:11:30.703249: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    }
   ],
   "source": [
    "re = my_tf_saved_model.predict(X_test_emdedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8248120951652527"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get 98 percentiles of re\n",
    "np.percentile(re, 98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
