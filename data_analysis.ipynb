{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install wordcloud Pillow nltk lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "import glob\n",
    "import zipfile\n",
    "import numpy\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "HOME_DIR = \"/home_remote\"\n",
    "HOME = \"/home/thi.tra.my.nguyen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datacollection(wpattern=\"./*.zip\"):\n",
    "    df = pd.DataFrame()\n",
    "    for g in glob.glob(wpattern):\n",
    "        zf = zipfile.ZipFile(g)\n",
    "        dfs = [pd.read_xml(zf.open(f)) for f in zf.namelist()[1:]]\n",
    "        df = pd.concat(dfs, ignore_index = True)\n",
    "\n",
    "    return df\n",
    "\n",
    "test = datacollection(os.path.join(HOME_DIR, \"eRisk2018training/2017_test/*.zip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_concat(path):\n",
    "    train_neg = pd.DataFrame()\n",
    "    for i in os.listdir(path):\n",
    "        #print(i)\n",
    "        allFiles = glob.glob(path+\"/\" + i+ \"/*.xml\")\n",
    "        dt = []\n",
    "        for x in allFiles:\n",
    "            try:\n",
    "                df = pd.read_xml(x)\n",
    "                dt.append(df)\n",
    "                #print(dt)\n",
    "                train_neg = pd.concat(dt)\n",
    "            except:\n",
    "                print(\"This file error \" + x)\n",
    "    return train_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg = file_concat(os.path.join(HOME_DIR, \"eRisk2018training/2017_train/negative_examples_anonymous_chunks\"))\n",
    "train_pos = file_concat(os.path.join(HOME_DIR, \"eRisk2018training/2017_train/positive_examples_anonymous_chunks\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = train_pos.dropna(subset=['TEXT'])\n",
    "train_neg = train_neg.dropna(subset=['TEXT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference: https://docs.python.org/3/library/re.html and \n",
    "#https://thinkinfi.com/fasttext-word-embeddings-python-implementation/\n",
    "def process_text(document):\n",
    "\n",
    "        # Remove extra white space from text\n",
    "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "         \n",
    "        # Remove all the special characters from text\n",
    "        document = re.sub(r'\\W', ' ', str(document))\n",
    " \n",
    "        # Remove all single characters from text\n",
    "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    " \n",
    "        # Converting to lowercase\n",
    "        document = document.lower()\n",
    " \n",
    "        return document\n",
    "\n",
    "#reference: https://www.nltk.org/api/nltk.tokenize.regexp.html\n",
    "cleaned_train = [process_text(sentence) for sentence in train_neg['TEXT'] if sentence.strip() !='']\n",
    "tokenizer_train = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S+')\n",
    "train_token_neg= [tokenizer_train.tokenize(sent) for sent in cleaned_train]\n",
    "\n",
    "cleaned_test = [process_text(sentence) for sentence in train_pos['TEXT'] if sentence.strip() !='']\n",
    "train_token_pos = [tokenizer_train.tokenize(sent) for sent in cleaned_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten the list train_token_neg and train_token_pos\n",
    "train_token_neg = [item for sublist in train_token_neg for item in sublist]\n",
    "train_token_pos = [item for sublist in train_token_pos for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords in train_token_neg and train_token_pos\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "train_token_neg = [w for w in train_token_neg if not w in stop_words]\n",
    "train_token_pos = [w for w in train_token_pos if not w in stop_words]\n",
    "\n",
    "#remove duplicates\n",
    "train_token_neg = list(dict.fromkeys(train_token_neg))\n",
    "train_token_pos = list(dict.fromkeys(train_token_pos))\n",
    "\n",
    "#remove words with length less than 3\n",
    "train_token_neg = [w for w in train_token_neg if len(w) >= 3]\n",
    "train_token_pos = [w for w in train_token_pos if len(w) >=3]\n",
    "\n",
    "#remove words are popular with frequency more than 10 times in both positive and negative\n",
    "freq_neg = pd.Series(' '.join(train_neg['TEXT']).split()).value_counts()[:50]\n",
    "freq_pos = pd.Series(' '.join(train_pos['TEXT']).split()).value_counts()[:50]\n",
    "freq_neg = list(freq_neg.index)\n",
    "freq_pos = list(freq_pos.index)\n",
    "train_token_neg = [w for w in train_token_neg if w not in freq_pos]\n",
    "train_token_pos = [w for w in train_token_pos if w not in freq_neg]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_token_neg = [w for w in train_token_neg if w not in ['college', 'hear','seem','cup']]\n",
    "train_token_pos = [w for w in train_token_pos if w not in ['call','economic','american','one','law']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordcloud for train_token_final and test_token_final\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "text1 = \" \".join(train_token_neg)\n",
    "text2 = \" \".join(train_token_pos)\n",
    "\n",
    "# Create and generate a word cloud image:\n",
    "font_path = os.path.join(HOME_DIR, 'fonts/Arial.ttf')\n",
    "wordcloud_neg= WordCloud(width=1000, height=600, background_color='white', font_path=font_path).generate(text1)\n",
    "wordcloud_pos= WordCloud(width=1000, height=600, background_color='white', font_path=font_path).generate(text2)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.imshow(wordcloud_neg, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.imshow(wordcloud_pos, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "#save picture as pdf\n",
    "wordcloud_neg.to_file(os.path.join(HOME,  'wordcloud_neg.pdf'))\n",
    "wordcloud_pos.to_file(os.path.join(HOME, 'wordcloud_pos.pdf'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uni, Bi Gram and Inforamtion Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel 'Python 3 (ipykernel)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. object unsubscribed"
     ]
    }
   ],
   "source": [
    "%store -r df\n",
    "%store -r df_neg\n",
    "df['Label'] = 1\n",
    "df_neg['Label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "df2= df.copy()\n",
    "def preprocess_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "df2['tokens'] = df2['Text'].apply(preprocess_text)\n",
    "\n",
    "# Create bigrams\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_documents(df2['tokens'].tolist())\n",
    "\n",
    "# Get the top 100 bigrams based on frequency\n",
    "top_100_bigrams = finder.nbest(bigram_measures.likelihood_ratio, 100)\n",
    "\n",
    "print(top_100_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bi-gram for text in df\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "def get_ngrams(text, n):\n",
    "    n_grams = ngrams(nltk.word_tokenize(text), n)\n",
    "    return [' '.join(grams) for grams in n_grams]\n",
    "pos_text = [ get_ngrams(text, 2) for text in df['Text']]\n",
    "neg_text = [ get_ngrams(text, 2) for text in df_neg['Text']]\n",
    "\n",
    "#remove the bi-grams in pos_text that are frequently appeared in neg_text\n",
    "pos_text = [item for sublist in pos_text for item in sublist]\n",
    "neg_text = [item for sublist in neg_text for item in sublist]\n",
    "pos_text = [w for w in pos_text if w not in neg_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bigrams\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_documents(pos_text)\n",
    "\n",
    "# Get the top 100 bigrams based on frequency\n",
    "top_100_bigrams = finder.nbest(bigram_measures.likelihood_ratio, 100)\n",
    "\n",
    "print(top_100_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
