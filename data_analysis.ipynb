{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# !{sys.executable} -m pip install wordcloud Pillow nltk lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "import glob\n",
    "import zipfile\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "HOME_DIR = \"/home_remote\"\n",
    "# HOME = \"/home/thi.tra.my.nguyen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datacollection(wpattern=\"./*.zip\"):\n",
    "    df = pd.DataFrame()\n",
    "    for g in glob.glob(wpattern):\n",
    "        zf = zipfile.ZipFile(g)\n",
    "        dfs = [pd.read_xml(zf.open(f)) for f in zf.namelist()[1:]]\n",
    "        df = pd.concat(dfs, ignore_index = True)\n",
    "\n",
    "    return df\n",
    "\n",
    "test = datacollection(os.path.join(HOME_DIR, \"eRisk2018training/2017_test/*.zip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_concat(path):\n",
    "    train_neg = pd.DataFrame()\n",
    "    for i in os.listdir(path):\n",
    "        #print(i)\n",
    "        allFiles = glob.glob(path+\"/\" + i+ \"/*.xml\")\n",
    "        dt = []\n",
    "        for x in allFiles:\n",
    "            try:\n",
    "                df = pd.read_xml(x)\n",
    "                dt.append(df)\n",
    "                #print(dt)\n",
    "                train_neg = pd.concat(dt)\n",
    "            except:\n",
    "                print(\"This file error \" + x)\n",
    "    return train_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg = file_concat(os.path.join(HOME_DIR, \"eRisk2018training/2017_train/negative_examples_anonymous_chunks\"))\n",
    "train_pos = file_concat(os.path.join(HOME_DIR, \"eRisk2018training/2017_train/positive_examples_anonymous_chunks\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = train_pos.dropna(subset=['TEXT'])\n",
    "train_neg = train_neg.dropna(subset=['TEXT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference: https://docs.python.org/3/library/re.html and \n",
    "#https://thinkinfi.com/fasttext-word-embeddings-python-implementation/\n",
    "def process_text(document):\n",
    "\n",
    "        # Remove extra white space from text\n",
    "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "         \n",
    "        # Remove all the special characters from text\n",
    "        document = re.sub(r'\\W', ' ', str(document))\n",
    " \n",
    "        # Remove all single characters from text\n",
    "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    " \n",
    "        # Converting to lowercase\n",
    "        document = document.lower()\n",
    " \n",
    "        return document\n",
    "\n",
    "#reference: https://www.nltk.org/api/nltk.tokenize.regexp.html\n",
    "cleaned_train = [process_text(sentence) for sentence in train_neg['TEXT'] if sentence.strip() !='']\n",
    "tokenizer_train = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S+')\n",
    "train_token_neg= [tokenizer_train.tokenize(sent) for sent in cleaned_train]\n",
    "\n",
    "cleaned_test = [process_text(sentence) for sentence in train_pos['TEXT'] if sentence.strip() !='']\n",
    "train_token_pos = [tokenizer_train.tokenize(sent) for sent in cleaned_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten the list train_token_neg and train_token_pos\n",
    "train_token_neg = [item for sublist in train_token_neg for item in sublist]\n",
    "train_token_pos = [item for sublist in train_token_pos for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords in train_token_neg and train_token_pos\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "train_token_neg = [w for w in train_token_neg if not w in stop_words]\n",
    "train_token_pos = [w for w in train_token_pos if not w in stop_words]\n",
    "\n",
    "#remove duplicates\n",
    "train_token_neg = list(dict.fromkeys(train_token_neg))\n",
    "train_token_pos = list(dict.fromkeys(train_token_pos))\n",
    "\n",
    "#remove words with length less than 3\n",
    "train_token_neg = [w for w in train_token_neg if len(w) >= 3]\n",
    "train_token_pos = [w for w in train_token_pos if len(w) >=3]\n",
    "\n",
    "#remove words are popular with frequency more than 10 times in both positive and negative\n",
    "freq_neg = pd.Series(' '.join(train_neg['TEXT']).split()).value_counts()[:50]\n",
    "freq_pos = pd.Series(' '.join(train_pos['TEXT']).split()).value_counts()[:50]\n",
    "freq_neg = list(freq_neg.index)\n",
    "freq_pos = list(freq_pos.index)\n",
    "train_token_neg = [w for w in train_token_neg if w not in freq_pos]\n",
    "train_token_pos = [w for w in train_token_pos if w not in freq_neg]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_token_neg = [w for w in train_token_neg if w not in ['college', 'hear','seem','cup']]\n",
    "train_token_pos = [w for w in train_token_pos if w not in ['call','economic','american','one','law']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordcloud for train_token_final and test_token_final\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "text1 = \" \".join(train_token_neg)\n",
    "text2 = \" \".join(train_token_pos)\n",
    "\n",
    "# Create and generate a word cloud image:\n",
    "font_path = os.path.join(HOME_DIR, 'fonts/Arial.ttf')\n",
    "wordcloud_neg= WordCloud(width=1000, height=600, background_color='white', font_path=font_path).generate(text1)\n",
    "wordcloud_pos= WordCloud(width=1000, height=600, background_color='white', font_path=font_path).generate(text2)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.imshow(wordcloud_neg, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.imshow(wordcloud_pos, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "#save picture as pdf\n",
    "wordcloud_neg.to_file(os.path.join(HOME,  'wordcloud_neg.pdf'))\n",
    "wordcloud_pos.to_file(os.path.join(HOME, 'wordcloud_pos.pdf'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uni, Bi Gram and Inforamtion Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r df\n",
    "%store -r df_neg\n",
    "df['Label'] = 1\n",
    "df_neg['Label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_subject4879</td>\n",
       "      <td>2014-02-15 05:03:58</td>\n",
       "      <td></td>\n",
       "      <td>You forgot it in people- broken social scene\\n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_subject4879</td>\n",
       "      <td>2014-02-11 04:55:50</td>\n",
       "      <td></td>\n",
       "      <td>fair enough, but i think the consequences of f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_subject4879</td>\n",
       "      <td>2014-02-11 04:36:02</td>\n",
       "      <td></td>\n",
       "      <td>yeah, i got 30/60. our marks were also 6 point...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_subject4879</td>\n",
       "      <td>2014-02-08 04:28:31</td>\n",
       "      <td></td>\n",
       "      <td>try redreader beta, not super popular, but awe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_subject4879</td>\n",
       "      <td>2014-02-05 01:21:55</td>\n",
       "      <td></td>\n",
       "      <td>i think you still get the breadth requirement....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21693</th>\n",
       "      <td>train_subject7367</td>\n",
       "      <td>2013-01-14 23:29:37</td>\n",
       "      <td></td>\n",
       "      <td>What restaurant? I live in phoenix.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21694</th>\n",
       "      <td>train_subject7367</td>\n",
       "      <td>2012-12-28 02:27:53</td>\n",
       "      <td></td>\n",
       "      <td>What is my ship doing on trees oh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21695</th>\n",
       "      <td>train_subject7367</td>\n",
       "      <td>2012-12-27 16:39:40</td>\n",
       "      <td></td>\n",
       "      <td>/hug \\nThis sounds like a dangerous position. ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21696</th>\n",
       "      <td>train_subject7367</td>\n",
       "      <td>2012-12-27 15:34:30</td>\n",
       "      <td></td>\n",
       "      <td>The thing about being diagnosed is that it sol...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21697</th>\n",
       "      <td>train_subject7367</td>\n",
       "      <td>2012-12-27 05:28:07</td>\n",
       "      <td></td>\n",
       "      <td>Thanks for the reading material :) I'm glad I ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21698 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    User                 Date Title  \\\n",
       "0      train_subject4879  2014-02-15 05:03:58         \n",
       "1      train_subject4879  2014-02-11 04:55:50         \n",
       "2      train_subject4879  2014-02-11 04:36:02         \n",
       "3      train_subject4879  2014-02-08 04:28:31         \n",
       "4      train_subject4879  2014-02-05 01:21:55         \n",
       "...                  ...                  ...   ...   \n",
       "21693  train_subject7367  2013-01-14 23:29:37         \n",
       "21694  train_subject7367  2012-12-28 02:27:53         \n",
       "21695  train_subject7367  2012-12-27 16:39:40         \n",
       "21696  train_subject7367  2012-12-27 15:34:30         \n",
       "21697  train_subject7367  2012-12-27 05:28:07         \n",
       "\n",
       "                                                    Text  Label  \n",
       "0      You forgot it in people- broken social scene\\n...      1  \n",
       "1      fair enough, but i think the consequences of f...      1  \n",
       "2      yeah, i got 30/60. our marks were also 6 point...      1  \n",
       "3      try redreader beta, not super popular, but awe...      1  \n",
       "4      i think you still get the breadth requirement....      1  \n",
       "...                                                  ...    ...  \n",
       "21693                What restaurant? I live in phoenix.      1  \n",
       "21694                  What is my ship doing on trees oh      1  \n",
       "21695  /hug \\nThis sounds like a dangerous position. ...      1  \n",
       "21696  The thing about being diagnosed is that it sol...      1  \n",
       "21697  Thanks for the reading material :) I'm glad I ...      1  \n",
       "\n",
       "[21698 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>train_subject8008</td>\n",
       "      <td>2013-09-17 16:20:51</td>\n",
       "      <td></td>\n",
       "      <td>Such precision.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>train_subject8008</td>\n",
       "      <td>2013-09-17 16:02:32</td>\n",
       "      <td></td>\n",
       "      <td>She looks ravishing.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>train_subject8008</td>\n",
       "      <td>2013-09-11 15:19:06</td>\n",
       "      <td></td>\n",
       "      <td>http://imgur.com/5zaHE9v</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>train_subject5448</td>\n",
       "      <td>2015-06-21 23:52:24</td>\n",
       "      <td></td>\n",
       "      <td>As everyone is saying, it sucks, but with time...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>train_subject5448</td>\n",
       "      <td>2015-06-16 18:18:32</td>\n",
       "      <td></td>\n",
       "      <td>The Doctor can get through anything. He always...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264165</th>\n",
       "      <td>train_subject2605</td>\n",
       "      <td>2013-08-01 04:20:15</td>\n",
       "      <td></td>\n",
       "      <td>Sweet! I'll start a thread on r/Atlanta this w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264166</th>\n",
       "      <td>train_subject2605</td>\n",
       "      <td>2013-07-31 21:26:42</td>\n",
       "      <td></td>\n",
       "      <td>I'm thinking about organizing a Reddit meet-up...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264167</th>\n",
       "      <td>train_subject2605</td>\n",
       "      <td>2013-07-31 21:20:51</td>\n",
       "      <td></td>\n",
       "      <td>I was considering organizing a Reddit meet-up ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264168</th>\n",
       "      <td>train_subject2605</td>\n",
       "      <td>2013-07-31 16:48:57</td>\n",
       "      <td></td>\n",
       "      <td>All I used was a pair of scissors and my hands...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264171</th>\n",
       "      <td>train_subject2605</td>\n",
       "      <td>2013-07-27 19:21:15</td>\n",
       "      <td></td>\n",
       "      <td>Bahahaha</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>182653 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     User                 Date Title  \\\n",
       "5       train_subject8008  2013-09-17 16:20:51         \n",
       "7       train_subject8008  2013-09-17 16:02:32         \n",
       "16      train_subject8008  2013-09-11 15:19:06         \n",
       "32      train_subject5448  2015-06-21 23:52:24         \n",
       "33      train_subject5448  2015-06-16 18:18:32         \n",
       "...                   ...                  ...   ...   \n",
       "264165  train_subject2605  2013-08-01 04:20:15         \n",
       "264166  train_subject2605  2013-07-31 21:26:42         \n",
       "264167  train_subject2605  2013-07-31 21:20:51         \n",
       "264168  train_subject2605  2013-07-31 16:48:57         \n",
       "264171  train_subject2605  2013-07-27 19:21:15         \n",
       "\n",
       "                                                     Text  Label  \n",
       "5                                         Such precision.      0  \n",
       "7                                    She looks ravishing.      0  \n",
       "16                               http://imgur.com/5zaHE9v      0  \n",
       "32      As everyone is saying, it sucks, but with time...      0  \n",
       "33      The Doctor can get through anything. He always...      0  \n",
       "...                                                   ...    ...  \n",
       "264165  Sweet! I'll start a thread on r/Atlanta this w...      0  \n",
       "264166  I'm thinking about organizing a Reddit meet-up...      0  \n",
       "264167  I was considering organizing a Reddit meet-up ...      0  \n",
       "264168  All I used was a pair of scissors and my hands...      0  \n",
       "264171                                           Bahahaha      0  \n",
       "\n",
       "[182653 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bi-gram for text in df\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "def get_ngrams(text, n):\n",
    "    n_grams = ngrams(nltk.word_tokenize(text), n)\n",
    "    return [' '.join(grams) for grams in n_grams]\n",
    "\n",
    "pos_text = [ get_ngrams(text, 2) for text in df['Text']]\n",
    "neg_text = [ get_ngrams(text, 2) for text in df_neg['Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('! -spins', '-spins WWE'),\n",
       " ('! ..', '.. a'),\n",
       " ('! ...................', '................... okay'),\n",
       " ('! /u/Cyrus224', 'lol Steam'),\n",
       " ('! 111', '111 !'),\n",
       " ('! 15/f', '15/f here'),\n",
       " ('! 150,000', '150,000 miles'),\n",
       " ('! 1e1', '1e1 !'),\n",
       " ('! 1s3USsajJK3InKeO5ET4N8iA', '1s3USsajJK3InKeO5ET4N8iA !'),\n",
       " ('! 2e0', '2e0 )'),\n",
       " ('! 3m2', '3m2 !'),\n",
       " ('! 3m4', '3m4 !'),\n",
       " ('! 608', '608 North'),\n",
       " ('! AJDKSJFS', 'Aha best'),\n",
       " ('! ALL', 'ALL MAKES'),\n",
       " ('! AM', 'AM !'),\n",
       " ('! AWESOME', 'AWESOME I'),\n",
       " ('! Add', \"Add 's\"),\n",
       " ('! Adding', 'Adding you'),\n",
       " ('! AmLactin', 'AmLactin is'),\n",
       " ('! Americans', 'Americans accept'),\n",
       " ('! Annoyingly', 'Annoyingly they'),\n",
       " ('! Asian', 'Asian women'),\n",
       " ('! Ask', 'Ask anyone'),\n",
       " ('! Asterisk', 'Asterisk !'),\n",
       " ('! Astro', 'Astro !'),\n",
       " ('! Awwexplosion', 'Awwexplosion ;'),\n",
       " ('! B-team', 'B-team !'),\n",
       " ('! BATMAN', 'BATMAN PREPTIME'),\n",
       " ('! BRING', 'BRING BACK'),\n",
       " ('! Balloons', 'Balloons scare'),\n",
       " ('! Beat', 'Beat women'),\n",
       " ('! Beautiful', 'Beautiful T'),\n",
       " ('! Beautifully', 'Beautifully photographed'),\n",
       " ('! Been', 'Been here'),\n",
       " ('! Being', 'Being on'),\n",
       " ('! Bitch', 'Bitch is'),\n",
       " ('! Brilliant', 'Brilliant !'),\n",
       " ('! Bro', 'Bro since'),\n",
       " ('! Butthurt', 'Butthurt .'),\n",
       " ('! CONFIDENCE', 'CONFIDENCE is'),\n",
       " ('! Canadian', 'Canadian French'),\n",
       " ('! Catch', 'Catch up'),\n",
       " ('! Cats', 'Cats are'),\n",
       " ('! Certain', 'Certain side'),\n",
       " ('! Certainly', 'Certainly hope'),\n",
       " ('! Continue', 'Continue on'),\n",
       " ('! Couple', 'Couple quick'),\n",
       " ('! Cozy', 'Cozy Rose'),\n",
       " ('! Cracking', 'Cracking body'),\n",
       " ('! Cup', 'Cup check'),\n",
       " ('! D2', 'D2 tomorrow'),\n",
       " ('! Dad', 'Dad eats'),\n",
       " ('! Daddy', 'Daddy loves'),\n",
       " ('! Diamond', 'Diamond ProX'),\n",
       " ('! Difference', 'Difference is'),\n",
       " ('! Dresses', 'Dresses that'),\n",
       " ('! Drill', 'Drill out'),\n",
       " ('! Due', 'Due beginning'),\n",
       " ('! ELECTION', 'ELECTION SEASON'),\n",
       " ('! Eat', 'Eat up'),\n",
       " ('! Edit-', 'Edit- I'),\n",
       " ('! Ever', 'Ever think'),\n",
       " ('! Everybody', 'Everybody gets'),\n",
       " ('! Everyone', 'Everyone tells'),\n",
       " ('! Exactly', 'Exactly my'),\n",
       " ('! FIREB', 'FIREB .....'),\n",
       " ('! Fairmount', 'Fairmount is'),\n",
       " ('! Fantastic', 'Fantastic picture'),\n",
       " ('! Female', 'Female Luc'),\n",
       " ('! Fox', 'Fox news-the'),\n",
       " ('! Fuck', 'Fuck them'),\n",
       " ('! Fun', 'Fun !'),\n",
       " ('! GET', 'GET OUT'),\n",
       " ('! GG', 'GG !'),\n",
       " ('! GO', 'GO !'),\n",
       " ('! GROOT', 'GROOT !'),\n",
       " ('! Giving', 'Giving my'),\n",
       " ('! Glasgow', 'Glasgow was'),\n",
       " ('! Gloomy', 'Gloomy days'),\n",
       " ('! Glorious', 'Glorious weather'),\n",
       " ('! Gold', 'Gold and'),\n",
       " ('! Halfway', 'Halfway in'),\n",
       " ('! High', 'High five'),\n",
       " ('! His', 'His Spiritual'),\n",
       " ('! Hoping', 'Hoping for'),\n",
       " ('! Humph', 'Humph !'),\n",
       " ('! Huntsmen', 'Huntsmen are'),\n",
       " ('! Hurray', 'Hurray !'),\n",
       " ('! Iet', 'Iet me'),\n",
       " ('! Imagining', 'Imagining myself'),\n",
       " ('! Je', 'Je suis'),\n",
       " ('! Jerome', 'Jerome lived'),\n",
       " ('! Know', 'Know what'),\n",
       " ('! LIKE', 'LIKE WTF'),\n",
       " ('! Left', 'Left handed'),\n",
       " ('! Life', 'Life really'),\n",
       " ('! Loved', 'Loved him'),\n",
       " ('! Luisa', 'Luisa was'),\n",
       " ('! MMR', 'MMR leader')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#flatten the list pos_text and neg_text\n",
    "pos_text = [item for sublist in pos_text for item in sublist]\n",
    "neg_text = [item for sublist in neg_text for item in sublist]\n",
    "\n",
    "# Remove top 1000 most common words in negative_words in positive_words\n",
    "freq_neg = pd.Series(' '.join(df_neg['Text']).split()).value_counts()[:2000]\n",
    "freq_neg = list(freq_neg.index)\n",
    "positive_words = [w for w in pos_text if w not in freq_neg]\n",
    "\n",
    "#calculate multual information for positive_words\n",
    "finder = BigramCollocationFinder.from_words(positive_words)\n",
    "measures = BigramAssocMeasures()\n",
    "finder.nbest(measures.pmi, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
